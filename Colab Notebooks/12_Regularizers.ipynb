{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_Regularizers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Trying out regularizers,no improvement, doesn't seem to work"
      ],
      "metadata": {
        "id": "Tjs3bAWlySjX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzBwr-VHGxbN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from itertools import chain\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "df = pd.read_csv(\"drive/MyDrive/Engineer's Project/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.pop('usa_cpi')\n",
        "df.pop('pol_cpi')\n",
        "df.pop('usa_inter')\n",
        "df.pop('pol_inter')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFp02HcTG08P",
        "outputId": "388b32c9-072b-426e-b69f-eb5107ba2283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       5.82\n",
              "1       5.82\n",
              "2       5.82\n",
              "3       5.82\n",
              "4       5.82\n",
              "        ... \n",
              "3537    4.83\n",
              "3538    4.83\n",
              "3539    4.83\n",
              "3540    4.83\n",
              "3541    4.83\n",
              "Name: pol_inter, Length: 3542, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.pop('Date')\n",
        "float_data = np.array(df).astype('float32')\n",
        "mean = float_data[:2501].mean(axis=0)\n",
        "std = float_data[:2501].std(axis=0)\n",
        "float_data -= mean\n",
        "float_data /= std"
      ],
      "metadata": {
        "id": "DApoWtojG141"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback = 30\n",
        "step = 1\n",
        "delay = 1\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "HggVJvK9G24u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,shuffle=False, batch_size=128, step=1):\n",
        "  if max_index is None:\n",
        "    max_index = len(data) - delay - 1\n",
        "  i = min_index + lookback\n",
        "  while 1:\n",
        "    if shuffle:\n",
        "      rows = np.random.randint(\n",
        "        min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "      if i + batch_size >= max_index:\n",
        "        i = min_index + lookback\n",
        "      rows = np.arange(i, min(i + batch_size, max_index))\n",
        "      i += len(rows)\n",
        "    samples = np.zeros((len(rows),lookback // step,data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      indices = range(rows[j] - lookback, rows[j], step)\n",
        "      samples[j] = data[indices]\n",
        "      targets[j] = data[rows[j] + delay][3] ##### HIT KURDE 1 to numer kolumny ktora przewiduje wtf\n",
        "    yield samples, targets"
      ],
      "metadata": {
        "id": "01lh_9vfG37v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=0,\n",
        "max_index=2501,\n",
        "shuffle=True,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "duu5e7ZiG5F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=2501,\n",
        "max_index=3001,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "HRMfoMQ3G6Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=3001,\n",
        "max_index=3541,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "QQIuVetwG7JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_steps = (3001 - 2501 - lookback)\n",
        "test_steps = (len(float_data) - 3001 - lookback)"
      ],
      "metadata": {
        "id": "1i-yGPzOG8BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best model, regularizer l1 in all layers, val loss constant  0.46,trying without activation in last layer - improvement"
      ],
      "metadata": {
        "id": "iKu3tmguJouu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(layers.LSTM(100,\n",
        "  activation='softmax',\n",
        "  dropout=0.1,\n",
        "  recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(5,\n",
        "  activation='softmax',))\n",
        "model.add(layers.Dense(1,))\n",
        "model.compile(optimizer= tf.keras.optimizers.RMSprop(), loss='mae')\n",
        "history = model.fit(train_gen,\n",
        "  steps_per_epoch=400,\n",
        "  epochs=200,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c68_IXmSHCUA",
        "outputId": "c50085f3-e096-4ce1-f6e6-bec6068269d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "400/400 [==============================] - 65s 153ms/step - loss: 0.8536 - val_loss: 1.0632\n",
            "Epoch 2/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.5309 - val_loss: 0.4910\n",
            "Epoch 3/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.2536 - val_loss: 0.1961\n",
            "Epoch 4/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.1414 - val_loss: 0.0805\n",
            "Epoch 5/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0980 - val_loss: 0.0752\n",
            "Epoch 6/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0861 - val_loss: 0.0709\n",
            "Epoch 7/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0809 - val_loss: 0.0699\n",
            "Epoch 8/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0780 - val_loss: 0.0645\n",
            "Epoch 9/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0759 - val_loss: 0.0611\n",
            "Epoch 10/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0750 - val_loss: 0.0599\n",
            "Epoch 11/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0735 - val_loss: 0.0685\n",
            "Epoch 12/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0730 - val_loss: 0.0581\n",
            "Epoch 13/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0725 - val_loss: 0.0614\n",
            "Epoch 14/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0724 - val_loss: 0.0647\n",
            "Epoch 15/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0711 - val_loss: 0.0612\n",
            "Epoch 16/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0712 - val_loss: 0.0631\n",
            "Epoch 17/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0708 - val_loss: 0.0678\n",
            "Epoch 18/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0708 - val_loss: 0.0699\n",
            "Epoch 19/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0706 - val_loss: 0.0718\n",
            "Epoch 20/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0703 - val_loss: 0.0557\n",
            "Epoch 21/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0708 - val_loss: 0.0590\n",
            "Epoch 22/200\n",
            "400/400 [==============================] - 51s 126ms/step - loss: 0.0704 - val_loss: 0.0566\n",
            "Epoch 23/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0701 - val_loss: 0.0622\n",
            "Epoch 24/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0699 - val_loss: 0.0655\n",
            "Epoch 25/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0697 - val_loss: 0.0557\n",
            "Epoch 26/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0695 - val_loss: 0.0549\n",
            "Epoch 27/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0693 - val_loss: 0.0564\n",
            "Epoch 28/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0693 - val_loss: 0.0638\n",
            "Epoch 29/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0697 - val_loss: 0.0536\n",
            "Epoch 30/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0690 - val_loss: 0.0567\n",
            "Epoch 31/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0693 - val_loss: 0.0757\n",
            "Epoch 32/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0692 - val_loss: 0.0765\n",
            "Epoch 33/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0690 - val_loss: 0.0560\n",
            "Epoch 34/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0689 - val_loss: 0.0710\n",
            "Epoch 35/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0682 - val_loss: 0.0677\n",
            "Epoch 36/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0686 - val_loss: 0.0638\n",
            "Epoch 37/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0689 - val_loss: 0.0596\n",
            "Epoch 38/200\n",
            "400/400 [==============================] - 50s 126ms/step - loss: 0.0688 - val_loss: 0.0605\n",
            "Epoch 39/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0686 - val_loss: 0.0582\n",
            "Epoch 40/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0684 - val_loss: 0.0596\n",
            "Epoch 41/200\n",
            "400/400 [==============================] - 50s 126ms/step - loss: 0.0686 - val_loss: 0.0577\n",
            "Epoch 42/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0681 - val_loss: 0.0613\n",
            "Epoch 43/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0679 - val_loss: 0.0527\n",
            "Epoch 44/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0684 - val_loss: 0.0579\n",
            "Epoch 45/200\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.0683 - val_loss: 0.0607\n",
            "Epoch 46/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0685 - val_loss: 0.0751\n",
            "Epoch 47/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0678 - val_loss: 0.0582\n",
            "Epoch 48/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0683 - val_loss: 0.0612\n",
            "Epoch 49/200\n",
            "400/400 [==============================] - 52s 131ms/step - loss: 0.0680 - val_loss: 0.0664\n",
            "Epoch 50/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0677 - val_loss: 0.0740\n",
            "Epoch 51/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0678 - val_loss: 0.0558\n",
            "Epoch 52/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0672 - val_loss: 0.0890\n",
            "Epoch 53/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0669 - val_loss: 0.0577\n",
            "Epoch 54/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0674 - val_loss: 0.0550\n",
            "Epoch 55/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0667 - val_loss: 0.0584\n",
            "Epoch 56/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0670 - val_loss: 0.0616\n",
            "Epoch 57/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0671 - val_loss: 0.0551\n",
            "Epoch 58/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0668 - val_loss: 0.0643\n",
            "Epoch 59/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0665 - val_loss: 0.0641\n",
            "Epoch 60/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0663 - val_loss: 0.0542\n",
            "Epoch 61/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0666 - val_loss: 0.0573\n",
            "Epoch 62/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0660 - val_loss: 0.0559\n",
            "Epoch 63/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0665 - val_loss: 0.0689\n",
            "Epoch 64/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0668 - val_loss: 0.0665\n",
            "Epoch 65/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0663 - val_loss: 0.0789\n",
            "Epoch 66/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0658 - val_loss: 0.0668\n",
            "Epoch 67/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0661 - val_loss: 0.0563\n",
            "Epoch 68/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0659 - val_loss: 0.0721\n",
            "Epoch 69/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0660 - val_loss: 0.0673\n",
            "Epoch 70/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0658 - val_loss: 0.0659\n",
            "Epoch 71/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0657 - val_loss: 0.0702\n",
            "Epoch 72/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0659 - val_loss: 0.0706\n",
            "Epoch 73/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0658 - val_loss: 0.0706\n",
            "Epoch 74/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0656 - val_loss: 0.0596\n",
            "Epoch 75/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0656 - val_loss: 0.0584\n",
            "Epoch 76/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0659 - val_loss: 0.0593\n",
            "Epoch 77/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0654 - val_loss: 0.0549\n",
            "Epoch 78/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0659 - val_loss: 0.0621\n",
            "Epoch 79/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0653 - val_loss: 0.0595\n",
            "Epoch 80/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0655 - val_loss: 0.0602\n",
            "Epoch 81/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0653 - val_loss: 0.0579\n",
            "Epoch 82/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0650 - val_loss: 0.0700\n",
            "Epoch 83/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0652 - val_loss: 0.0587\n",
            "Epoch 84/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0655 - val_loss: 0.0609\n",
            "Epoch 85/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0652 - val_loss: 0.0655\n",
            "Epoch 86/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0651 - val_loss: 0.0657\n",
            "Epoch 87/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0650 - val_loss: 0.0550\n",
            "Epoch 88/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0650 - val_loss: 0.0588\n",
            "Epoch 89/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0648 - val_loss: 0.0821\n",
            "Epoch 90/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0648 - val_loss: 0.0539\n",
            "Epoch 91/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0651 - val_loss: 0.0700\n",
            "Epoch 92/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0648 - val_loss: 0.0716\n",
            "Epoch 93/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0652 - val_loss: 0.0576\n",
            "Epoch 94/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0648 - val_loss: 0.0609\n",
            "Epoch 95/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0645 - val_loss: 0.0724\n",
            "Epoch 96/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0638 - val_loss: 0.0558\n",
            "Epoch 97/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0649 - val_loss: 0.0616\n",
            "Epoch 98/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0642 - val_loss: 0.0585\n",
            "Epoch 99/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0647 - val_loss: 0.0589\n",
            "Epoch 100/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0645 - val_loss: 0.0564\n",
            "Epoch 101/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0643 - val_loss: 0.0677\n",
            "Epoch 102/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0646 - val_loss: 0.0613\n",
            "Epoch 103/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0639 - val_loss: 0.0652\n",
            "Epoch 104/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0641 - val_loss: 0.0607\n",
            "Epoch 105/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0638 - val_loss: 0.0641\n",
            "Epoch 106/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0641 - val_loss: 0.0553\n",
            "Epoch 107/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0635 - val_loss: 0.0714\n",
            "Epoch 108/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0644 - val_loss: 0.0552\n",
            "Epoch 109/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0645 - val_loss: 0.0559\n",
            "Epoch 110/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0640 - val_loss: 0.0562\n",
            "Epoch 111/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0642 - val_loss: 0.0571\n",
            "Epoch 112/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0638 - val_loss: 0.0675\n",
            "Epoch 113/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0639 - val_loss: 0.0653\n",
            "Epoch 114/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0637 - val_loss: 0.0609\n",
            "Epoch 115/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0639 - val_loss: 0.0586\n",
            "Epoch 116/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0641 - val_loss: 0.0624\n",
            "Epoch 117/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0636 - val_loss: 0.0557\n",
            "Epoch 118/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0636 - val_loss: 0.0595\n",
            "Epoch 119/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0634 - val_loss: 0.0668\n",
            "Epoch 120/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0635 - val_loss: 0.0571\n",
            "Epoch 121/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0635 - val_loss: 0.0669\n",
            "Epoch 122/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0634 - val_loss: 0.0609\n",
            "Epoch 123/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0635 - val_loss: 0.0641\n",
            "Epoch 124/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0636 - val_loss: 0.0704\n",
            "Epoch 125/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0629 - val_loss: 0.0654\n",
            "Epoch 126/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0630 - val_loss: 0.0595\n",
            "Epoch 127/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0631 - val_loss: 0.0641\n",
            "Epoch 128/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0629 - val_loss: 0.0577\n",
            "Epoch 129/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0631 - val_loss: 0.0587\n",
            "Epoch 130/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0629 - val_loss: 0.0613\n",
            "Epoch 131/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0628 - val_loss: 0.0607\n",
            "Epoch 132/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0631 - val_loss: 0.0633\n",
            "Epoch 133/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0628 - val_loss: 0.0639\n",
            "Epoch 134/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0625 - val_loss: 0.0587\n",
            "Epoch 135/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0631 - val_loss: 0.0797\n",
            "Epoch 136/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0629 - val_loss: 0.0748\n",
            "Epoch 137/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0626 - val_loss: 0.0577\n",
            "Epoch 138/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0626 - val_loss: 0.0589\n",
            "Epoch 139/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0626 - val_loss: 0.0651\n",
            "Epoch 140/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0625 - val_loss: 0.0574\n",
            "Epoch 141/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0621 - val_loss: 0.0636\n",
            "Epoch 142/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0629 - val_loss: 0.0597\n",
            "Epoch 143/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0629 - val_loss: 0.0721\n",
            "Epoch 144/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0621 - val_loss: 0.0609\n",
            "Epoch 145/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0626 - val_loss: 0.0617\n",
            "Epoch 146/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0628 - val_loss: 0.0579\n",
            "Epoch 147/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0624 - val_loss: 0.0631\n",
            "Epoch 148/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0625 - val_loss: 0.0622\n",
            "Epoch 149/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0620 - val_loss: 0.0670\n",
            "Epoch 150/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0619 - val_loss: 0.0642\n",
            "Epoch 151/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0622 - val_loss: 0.0611\n",
            "Epoch 152/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0622 - val_loss: 0.0629\n",
            "Epoch 153/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0623 - val_loss: 0.0607\n",
            "Epoch 154/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0624 - val_loss: 0.0682\n",
            "Epoch 155/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0622 - val_loss: 0.0620\n",
            "Epoch 156/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0622 - val_loss: 0.0635\n",
            "Epoch 157/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0623 - val_loss: 0.0652\n",
            "Epoch 158/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0620 - val_loss: 0.0722\n",
            "Epoch 159/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0623 - val_loss: 0.0646\n",
            "Epoch 160/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0618 - val_loss: 0.0667\n",
            "Epoch 161/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0621 - val_loss: 0.0667\n",
            "Epoch 162/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0622 - val_loss: 0.0735\n",
            "Epoch 163/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0621 - val_loss: 0.0611\n",
            "Epoch 164/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0625 - val_loss: 0.0647\n",
            "Epoch 165/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0618 - val_loss: 0.0677\n",
            "Epoch 166/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0619 - val_loss: 0.0659\n",
            "Epoch 167/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0618 - val_loss: 0.0604\n",
            "Epoch 168/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0620 - val_loss: 0.0641\n",
            "Epoch 169/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0615 - val_loss: 0.0636\n",
            "Epoch 170/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0611 - val_loss: 0.0629\n",
            "Epoch 171/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0612 - val_loss: 0.0641\n",
            "Epoch 172/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0618 - val_loss: 0.0650\n",
            "Epoch 173/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0612 - val_loss: 0.0633\n",
            "Epoch 174/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0614 - val_loss: 0.0609\n",
            "Epoch 175/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0614 - val_loss: 0.0634\n",
            "Epoch 176/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0614 - val_loss: 0.0640\n",
            "Epoch 177/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0615 - val_loss: 0.0666\n",
            "Epoch 178/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0612 - val_loss: 0.0617\n",
            "Epoch 179/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0618 - val_loss: 0.0633\n",
            "Epoch 180/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0615 - val_loss: 0.0641\n",
            "Epoch 181/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0610 - val_loss: 0.0685\n",
            "Epoch 182/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0618 - val_loss: 0.0629\n",
            "Epoch 183/200\n",
            "400/400 [==============================] - 52s 131ms/step - loss: 0.0613 - val_loss: 0.0629\n",
            "Epoch 184/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0615 - val_loss: 0.0653\n",
            "Epoch 185/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0612 - val_loss: 0.0636\n",
            "Epoch 186/200\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.0611 - val_loss: 0.0628\n",
            "Epoch 187/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0609 - val_loss: 0.0645\n",
            "Epoch 188/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0611 - val_loss: 0.0654\n",
            "Epoch 189/200\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.0608 - val_loss: 0.0666\n",
            "Epoch 190/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0611 - val_loss: 0.0641\n",
            "Epoch 191/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0614 - val_loss: 0.0639\n",
            "Epoch 192/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0610 - val_loss: 0.0677\n",
            "Epoch 193/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0613 - val_loss: 0.0650\n",
            "Epoch 194/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0610 - val_loss: 0.0632\n",
            "Epoch 195/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0610 - val_loss: 0.0677\n",
            "Epoch 196/200\n",
            "400/400 [==============================] - 50s 125ms/step - loss: 0.0617 - val_loss: 0.0653\n",
            "Epoch 197/200\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.0608 - val_loss: 0.0658\n",
            "Epoch 198/200\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.0613 - val_loss: 0.0648\n",
            "Epoch 199/200\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.0611 - val_loss: 0.0738\n",
            "Epoch 200/200\n",
            "400/400 [==============================] - 51s 126ms/step - loss: 0.0608 - val_loss: 0.0652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "IA__gKCk4y7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model.evaluate(test_gen, steps = test_steps)\n",
        "eval *std[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzY72QBgHEj6",
        "outputId": "d37379b3-cba0-4f47-cabc-136ea85d846a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511/511 [==============================] - 13s 25ms/step - loss: 0.0675\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02519075519697922"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss,softmax, RMSprop, regularization L1 CLOSING PRICE PREDICTION')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "9Z0vgsuBHFr6",
        "outputId": "ae97b995-d8f0-409d-8c97-461a17111289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150 is too much, trying lower\n"
      ],
      "metadata": {
        "id": "h1UG0DvZysz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tf.keras.models.Sequential()\n",
        "model1.add(layers.LSTM(150,\n",
        "  activation='softmax',\n",
        "  dropout=0.1,\n",
        "  recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model1.add(layers.Dense(5,\n",
        "  activation='softmax',))\n",
        "model1.add(layers.Dense(1,))\n",
        "model1.compile(optimizer= tf.keras.optimizers.RMSprop(), loss='mae')\n",
        "history1 = model1.fit(train_gen,\n",
        "  steps_per_epoch=400,\n",
        "  epochs=200,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Udtok9LNINt",
        "outputId": "69e22c07-67b8-4650-d590-35cfab27a6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "400/400 [==============================] - 90s 218ms/step - loss: 0.8170 - val_loss: 0.9215\n",
            "Epoch 2/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.3976 - val_loss: 0.2067\n",
            "Epoch 3/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.1652 - val_loss: 0.1575\n",
            "Epoch 4/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.1074 - val_loss: 0.1076\n",
            "Epoch 5/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0906 - val_loss: 0.0807\n",
            "Epoch 6/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0837 - val_loss: 0.0680\n",
            "Epoch 7/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0799 - val_loss: 0.0631\n",
            "Epoch 8/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0782 - val_loss: 0.0609\n",
            "Epoch 9/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0765 - val_loss: 0.0565\n",
            "Epoch 10/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0745 - val_loss: 0.0685\n",
            "Epoch 11/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0732 - val_loss: 0.0651\n",
            "Epoch 12/200\n",
            "400/400 [==============================] - 87s 216ms/step - loss: 0.0729 - val_loss: 0.0672\n",
            "Epoch 13/200\n",
            "400/400 [==============================] - 86s 216ms/step - loss: 0.0718 - val_loss: 0.0590\n",
            "Epoch 14/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0716 - val_loss: 0.0709\n",
            "Epoch 15/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0710 - val_loss: 0.0602\n",
            "Epoch 16/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0711 - val_loss: 0.0654\n",
            "Epoch 17/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0712 - val_loss: 0.0704\n",
            "Epoch 18/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0707 - val_loss: 0.0652\n",
            "Epoch 19/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0703 - val_loss: 0.0715\n",
            "Epoch 20/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0701 - val_loss: 0.0660\n",
            "Epoch 21/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0703 - val_loss: 0.0710\n",
            "Epoch 22/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0700 - val_loss: 0.0670\n",
            "Epoch 23/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0698 - val_loss: 0.0708\n",
            "Epoch 24/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0696 - val_loss: 0.0678\n",
            "Epoch 25/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0693 - val_loss: 0.0626\n",
            "Epoch 26/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0691 - val_loss: 0.0734\n",
            "Epoch 27/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0690 - val_loss: 0.0662\n",
            "Epoch 28/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0693 - val_loss: 0.0769\n",
            "Epoch 29/200\n",
            "400/400 [==============================] - 89s 221ms/step - loss: 0.0687 - val_loss: 0.0886\n",
            "Epoch 30/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0695 - val_loss: 0.0762\n",
            "Epoch 31/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0689 - val_loss: 0.0760\n",
            "Epoch 32/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0690 - val_loss: 0.0723\n",
            "Epoch 33/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0686 - val_loss: 0.0728\n",
            "Epoch 34/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0685 - val_loss: 0.0745\n",
            "Epoch 35/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0681 - val_loss: 0.0819\n",
            "Epoch 36/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0684 - val_loss: 0.0790\n",
            "Epoch 37/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0677 - val_loss: 0.0770\n",
            "Epoch 38/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0684 - val_loss: 0.0743\n",
            "Epoch 39/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0677 - val_loss: 0.0696\n",
            "Epoch 40/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0679 - val_loss: 0.0828\n",
            "Epoch 41/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0680 - val_loss: 0.0858\n",
            "Epoch 42/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0681 - val_loss: 0.0729\n",
            "Epoch 43/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0681 - val_loss: 0.0786\n",
            "Epoch 44/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0676 - val_loss: 0.0877\n",
            "Epoch 45/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0675 - val_loss: 0.0857\n",
            "Epoch 46/200\n",
            "400/400 [==============================] - 86s 215ms/step - loss: 0.0673 - val_loss: 0.0919\n",
            "Epoch 47/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0674 - val_loss: 0.0858\n",
            "Epoch 48/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0672 - val_loss: 0.0840\n",
            "Epoch 49/200\n",
            "400/400 [==============================] - 86s 216ms/step - loss: 0.0676 - val_loss: 0.0841\n",
            "Epoch 50/200\n",
            "400/400 [==============================] - 86s 216ms/step - loss: 0.0669 - val_loss: 0.0805\n",
            "Epoch 51/200\n",
            "400/400 [==============================] - 86s 216ms/step - loss: 0.0668 - val_loss: 0.0778\n",
            "Epoch 52/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0671 - val_loss: 0.0867\n",
            "Epoch 53/200\n",
            "400/400 [==============================] - 86s 215ms/step - loss: 0.0663 - val_loss: 0.0747\n",
            "Epoch 54/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0669 - val_loss: 0.0729\n",
            "Epoch 55/200\n",
            "400/400 [==============================] - 86s 216ms/step - loss: 0.0669 - val_loss: 0.0770\n",
            "Epoch 56/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0673 - val_loss: 0.0909\n",
            "Epoch 57/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0665 - val_loss: 0.0799\n",
            "Epoch 58/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0667 - val_loss: 0.0820\n",
            "Epoch 59/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0669 - val_loss: 0.0852\n",
            "Epoch 60/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0669 - val_loss: 0.0819\n",
            "Epoch 61/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0667 - val_loss: 0.0680\n",
            "Epoch 62/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0664 - val_loss: 0.0888\n",
            "Epoch 63/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0667 - val_loss: 0.0720\n",
            "Epoch 64/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0661 - val_loss: 0.0641\n",
            "Epoch 65/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0661 - val_loss: 0.0738\n",
            "Epoch 66/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0668 - val_loss: 0.0833\n",
            "Epoch 67/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0661 - val_loss: 0.0951\n",
            "Epoch 68/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0664 - val_loss: 0.0738\n",
            "Epoch 69/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0658 - val_loss: 0.0712\n",
            "Epoch 70/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0662 - val_loss: 0.0742\n",
            "Epoch 71/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0663 - val_loss: 0.0796\n",
            "Epoch 72/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0656 - val_loss: 0.0746\n",
            "Epoch 73/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0663 - val_loss: 0.0924\n",
            "Epoch 74/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0659 - val_loss: 0.0841\n",
            "Epoch 75/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0658 - val_loss: 0.0762\n",
            "Epoch 76/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0653 - val_loss: 0.0760\n",
            "Epoch 77/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0656 - val_loss: 0.0884\n",
            "Epoch 78/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0656 - val_loss: 0.0865\n",
            "Epoch 79/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0654 - val_loss: 0.0825\n",
            "Epoch 80/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0655 - val_loss: 0.0769\n",
            "Epoch 81/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0653 - val_loss: 0.0859\n",
            "Epoch 82/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0657 - val_loss: 0.0741\n",
            "Epoch 83/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0655 - val_loss: 0.0722\n",
            "Epoch 84/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0649 - val_loss: 0.0852\n",
            "Epoch 85/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0651 - val_loss: 0.0747\n",
            "Epoch 86/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0656 - val_loss: 0.0719\n",
            "Epoch 87/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0657 - val_loss: 0.0789\n",
            "Epoch 88/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0653 - val_loss: 0.0650\n",
            "Epoch 89/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0647 - val_loss: 0.0641\n",
            "Epoch 90/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0651 - val_loss: 0.0769\n",
            "Epoch 91/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0649 - val_loss: 0.0629\n",
            "Epoch 92/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0648 - val_loss: 0.0702\n",
            "Epoch 93/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0650 - val_loss: 0.0773\n",
            "Epoch 94/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0648 - val_loss: 0.0690\n",
            "Epoch 95/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0650 - val_loss: 0.0717\n",
            "Epoch 96/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0648 - val_loss: 0.0748\n",
            "Epoch 97/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0648 - val_loss: 0.0799\n",
            "Epoch 98/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0648 - val_loss: 0.0750\n",
            "Epoch 99/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0644 - val_loss: 0.0652\n",
            "Epoch 100/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0646 - val_loss: 0.0637\n",
            "Epoch 101/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0645 - val_loss: 0.0762\n",
            "Epoch 102/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0646 - val_loss: 0.0856\n",
            "Epoch 103/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0647 - val_loss: 0.0748\n",
            "Epoch 104/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0644 - val_loss: 0.0787\n",
            "Epoch 105/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0644 - val_loss: 0.0848\n",
            "Epoch 106/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0642 - val_loss: 0.0659\n",
            "Epoch 107/200\n",
            "400/400 [==============================] - 86s 216ms/step - loss: 0.0643 - val_loss: 0.0716\n",
            "Epoch 108/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0646 - val_loss: 0.0701\n",
            "Epoch 109/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0646 - val_loss: 0.0701\n",
            "Epoch 110/200\n",
            "400/400 [==============================] - 87s 219ms/step - loss: 0.0643 - val_loss: 0.0747\n",
            "Epoch 111/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0642 - val_loss: 0.0754\n",
            "Epoch 112/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0643 - val_loss: 0.0854\n",
            "Epoch 113/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0646 - val_loss: 0.0774\n",
            "Epoch 114/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0644 - val_loss: 0.0976\n",
            "Epoch 115/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0642 - val_loss: 0.1197\n",
            "Epoch 116/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0641 - val_loss: 0.1023\n",
            "Epoch 117/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0637 - val_loss: 0.0938\n",
            "Epoch 118/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0636 - val_loss: 0.0931\n",
            "Epoch 119/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0640 - val_loss: 0.0958\n",
            "Epoch 120/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0639 - val_loss: 0.0932\n",
            "Epoch 121/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0639 - val_loss: 0.1039\n",
            "Epoch 122/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0636 - val_loss: 0.1465\n",
            "Epoch 123/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0641 - val_loss: 0.1282\n",
            "Epoch 124/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0637 - val_loss: 0.1427\n",
            "Epoch 125/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0637 - val_loss: 0.1366\n",
            "Epoch 126/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0640 - val_loss: 0.1314\n",
            "Epoch 127/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0639 - val_loss: 0.1514\n",
            "Epoch 128/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0638 - val_loss: 0.1580\n",
            "Epoch 129/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0638 - val_loss: 0.1511\n",
            "Epoch 130/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0636 - val_loss: 0.2004\n",
            "Epoch 131/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0638 - val_loss: 0.1648\n",
            "Epoch 132/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0635 - val_loss: 0.1653\n",
            "Epoch 133/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0633 - val_loss: 0.1813\n",
            "Epoch 134/200\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.0636 - val_loss: 0.2016\n",
            "Epoch 135/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0633 - val_loss: 0.1846\n",
            "Epoch 136/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0636 - val_loss: 0.1900\n",
            "Epoch 137/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0633 - val_loss: 0.2281\n",
            "Epoch 138/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0635 - val_loss: 0.1890\n",
            "Epoch 139/200\n",
            "400/400 [==============================] - 90s 225ms/step - loss: 0.0633 - val_loss: 0.1748\n",
            "Epoch 140/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0631 - val_loss: 0.2255\n",
            "Epoch 141/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0629 - val_loss: 0.2041\n",
            "Epoch 142/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0631 - val_loss: 0.2238\n",
            "Epoch 143/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0632 - val_loss: 0.2126\n",
            "Epoch 144/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0630 - val_loss: 0.2271\n",
            "Epoch 145/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0631 - val_loss: 0.2476\n",
            "Epoch 146/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0629 - val_loss: 0.2320\n",
            "Epoch 147/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0630 - val_loss: 0.2349\n",
            "Epoch 148/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0630 - val_loss: 0.2678\n",
            "Epoch 149/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0629 - val_loss: 0.2498\n",
            "Epoch 150/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0630 - val_loss: 0.2721\n",
            "Epoch 151/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0628 - val_loss: 0.2349\n",
            "Epoch 152/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0628 - val_loss: 0.3028\n",
            "Epoch 153/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0626 - val_loss: 0.3162\n",
            "Epoch 154/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0627 - val_loss: 0.3365\n",
            "Epoch 155/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0626 - val_loss: 0.3056\n",
            "Epoch 156/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0627 - val_loss: 0.2933\n",
            "Epoch 157/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0626 - val_loss: 0.2910\n",
            "Epoch 158/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0625 - val_loss: 0.3081\n",
            "Epoch 159/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0628 - val_loss: 0.2977\n",
            "Epoch 160/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0624 - val_loss: 0.3506\n",
            "Epoch 161/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0625 - val_loss: 0.4344\n",
            "Epoch 162/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0628 - val_loss: 0.3467\n",
            "Epoch 163/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0627 - val_loss: 0.3461\n",
            "Epoch 164/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0627 - val_loss: 0.3654\n",
            "Epoch 165/200\n",
            "400/400 [==============================] - 87s 218ms/step - loss: 0.0624 - val_loss: 0.4086\n",
            "Epoch 166/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0619 - val_loss: 0.3422\n",
            "Epoch 167/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0623 - val_loss: 0.3859\n",
            "Epoch 168/200\n",
            "400/400 [==============================] - 90s 224ms/step - loss: 0.0621 - val_loss: 0.3663\n",
            "Epoch 169/200\n",
            "400/400 [==============================] - 89s 223ms/step - loss: 0.0622 - val_loss: 0.3712\n",
            "Epoch 170/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0623 - val_loss: 0.3666\n",
            "Epoch 171/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0621 - val_loss: 0.3675\n",
            "Epoch 172/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0623 - val_loss: 0.3976\n",
            "Epoch 173/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0625 - val_loss: 0.4500\n",
            "Epoch 174/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0620 - val_loss: 0.4705\n",
            "Epoch 175/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0622 - val_loss: 0.4888\n",
            "Epoch 176/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0617 - val_loss: 0.4409\n",
            "Epoch 177/200\n",
            "400/400 [==============================] - 88s 220ms/step - loss: 0.0622 - val_loss: 0.3983\n",
            "Epoch 178/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0617 - val_loss: 0.4449\n",
            "Epoch 179/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0618 - val_loss: 0.4248\n",
            "Epoch 180/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0620 - val_loss: 0.4246\n",
            "Epoch 181/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0619 - val_loss: 0.4134\n",
            "Epoch 182/200\n",
            "400/400 [==============================] - 89s 222ms/step - loss: 0.0621 - val_loss: 0.4765\n",
            "Epoch 183/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0621 - val_loss: 0.4885\n",
            "Epoch 184/200\n",
            "400/400 [==============================] - 88s 221ms/step - loss: 0.0623 - val_loss: 0.4490\n",
            "Epoch 185/200\n",
            "400/400 [==============================] - 88s 219ms/step - loss: 0.0622 - val_loss: 0.4895\n",
            "Epoch 186/200\n",
            "344/400 [========================>.....] - ETA: 9s - loss: 0.0619 "
          ]
        }
      ]
    }
  ]
}