{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCQ7MWL9EB5h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from itertools import chain\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#import keras_tuner as kt\n",
        "df = pd.read_csv(\"drive/MyDrive/Engineer's Project/output_eur_pln.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYEcjbqBEGfp",
        "outputId": "176ea0ba-99fb-4a13-df4d-8a8846fdd2ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2010.11.15\n",
              "1       2010.11.16\n",
              "2       2010.11.17\n",
              "3       2010.11.18\n",
              "4       2010.11.19\n",
              "           ...    \n",
              "3537    2022.03.27\n",
              "3538    2022.03.28\n",
              "3539    2022.03.29\n",
              "3540    2022.03.30\n",
              "3541    2022.03.31\n",
              "Name: Date, Length: 3542, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#df.pop('usa_cpi')\n",
        "#df.pop('pol_cpi')\n",
        "#df.pop('usa_inter')\n",
        "#df.pop('pol_inter')\n",
        "df.pop('Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kpn6cgfEJAH"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_numpy = df.to_numpy() \n",
        "#len(df_numpy[:2500])\n",
        "scaler1 = scaler.fit(df_numpy[:3000])\n",
        "df_scalled = scaler1.transform(df_numpy)\n",
        "#df_scaled_all = scaler.\n",
        "df_scalled = pd.DataFrame(df_scalled, columns=[\n",
        "  'Opening', 'High', 'Low', 'Closing','Momentum', 'Range', 'ohlc'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(date,df[\"Closing\"], marker='o')\n",
        "\n",
        "# Labelling \n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Vurreny\")\n",
        "plt.title(\"Pandas Time Series Plot\")\n",
        "\n",
        "# Display\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "FRxa6znsSyFO",
        "outputId": "ff287a99-f40e-4c11-c3d0-1e17f72c3733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "date = pd.to_datetime(df[\"Date\"])"
      ],
      "metadata": {
        "id": "mchOmVIwT9cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scalled.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "knl7vtekUAOs",
        "outputId": "64b1f888-605a-4205-f72a-59ca4d26140b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Opening      High       Low   Closing  Momentum     Range      ohlc\n",
              "0  0.136591  0.112886  0.153342  0.136824  0.430161  0.035787  0.135242\n",
              "1  0.119463  0.128421  0.126540  0.119453  0.545131  0.227246  0.123938\n",
              "2  0.133874  0.139684  0.128807  0.133960  0.499848  0.268997  0.134608\n",
              "3  0.150008  0.140201  0.159742  0.150048  0.521365  0.133007  0.150470\n",
              "4  0.125477  0.120912  0.134007  0.125477  0.540389  0.159251  0.126881"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e376c36-182b-4311-8bf9-54d38ee259ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Opening</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Closing</th>\n",
              "      <th>Momentum</th>\n",
              "      <th>Range</th>\n",
              "      <th>ohlc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.136591</td>\n",
              "      <td>0.112886</td>\n",
              "      <td>0.153342</td>\n",
              "      <td>0.136824</td>\n",
              "      <td>0.430161</td>\n",
              "      <td>0.035787</td>\n",
              "      <td>0.135242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.119463</td>\n",
              "      <td>0.128421</td>\n",
              "      <td>0.126540</td>\n",
              "      <td>0.119453</td>\n",
              "      <td>0.545131</td>\n",
              "      <td>0.227246</td>\n",
              "      <td>0.123938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.133874</td>\n",
              "      <td>0.139684</td>\n",
              "      <td>0.128807</td>\n",
              "      <td>0.133960</td>\n",
              "      <td>0.499848</td>\n",
              "      <td>0.268997</td>\n",
              "      <td>0.134608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.150008</td>\n",
              "      <td>0.140201</td>\n",
              "      <td>0.159742</td>\n",
              "      <td>0.150048</td>\n",
              "      <td>0.521365</td>\n",
              "      <td>0.133007</td>\n",
              "      <td>0.150470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125477</td>\n",
              "      <td>0.120912</td>\n",
              "      <td>0.134007</td>\n",
              "      <td>0.125477</td>\n",
              "      <td>0.540389</td>\n",
              "      <td>0.159251</td>\n",
              "      <td>0.126881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e376c36-182b-4311-8bf9-54d38ee259ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e376c36-182b-4311-8bf9-54d38ee259ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e376c36-182b-4311-8bf9-54d38ee259ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bj1udUdEKGL"
      },
      "outputs": [],
      "source": [
        "lookback = 15 #15 for 1day\n",
        "step = 1\n",
        "delay = 0 #0 for 1day\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6xaTInqELWH"
      },
      "outputs": [],
      "source": [
        "float_data = np.array(df_scalled).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(float_data[3001:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOCdigRzNsD2",
        "outputId": "0b96d40c-ecfe-434b-b94c-70376e682195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "541"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY_O0oi7ENwD"
      },
      "outputs": [],
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,shuffle=False, batch_size=128, step=1):\n",
        "  if max_index is None:\n",
        "    max_index = len(data) - delay - 1\n",
        "  i = min_index + lookback\n",
        "  while 1:\n",
        "    if shuffle:\n",
        "      rows = np.random.randint(\n",
        "        min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "      if i + batch_size >= max_index:\n",
        "        i = min_index + lookback\n",
        "      rows = np.arange(i, min(i + batch_size, max_index))\n",
        "      i += len(rows)\n",
        "    samples = np.zeros((len(rows),lookback // step,data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      indices = range(rows[j] - lookback, rows[j], step)\n",
        "      samples[j] = data[indices]\n",
        "      targets[j] = data[rows[j] + delay][3] \n",
        "    yield samples, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk-ldxRtEP2u"
      },
      "outputs": [],
      "source": [
        "train_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=0,\n",
        "max_index=2500-delay,\n",
        "#shuffle=True,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbs-odkeEQ6T"
      },
      "outputs": [],
      "source": [
        "val_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=2501,\n",
        "max_index=3000-delay,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dvMBTmsER6J"
      },
      "outputs": [],
      "source": [
        "test_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=3001,\n",
        "max_index=3541-delay,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(float_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lhAZnGBwjNL",
        "outputId": "dc667fae-09b5-4bfa-c32d-e360df801244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3542"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R2g9kpFw8p7",
        "outputId": "7ff3ee2d-3ba2-45a0-fb7a-d6a6fe91ac50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "525"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfFHpPVJES_p"
      },
      "outputs": [],
      "source": [
        "train_steps = (2500 - lookback)\n",
        "val_steps = (3000 - 2501 - lookback)\n",
        "test_steps = (3541 - 3001 - lookback)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(layers.GRU(140,\n",
        "  activation='sigmoid',\n",
        "  #dropout=0.1,\n",
        "  #recurrent_dropout=0.1,\n",
        "  return_sequences=True,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(120,\n",
        "  activation='sigmoid',))\n",
        "model.add(layers.GRU(4,\n",
        "  activation='sigmoid',\n",
        "  #dropout=0.1,\n",
        "  #recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1,\n",
        "  activation='sigmoid'))\n",
        "model.compile(optimizer= tf.keras.optimizers.RMSprop(0.01), loss='mae')\n",
        "history = model.fit(train_gen,\n",
        "  steps_per_epoch=100,\n",
        "  epochs=200,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew8N2dpg0ChU",
        "outputId": "79777fcc-e604-4604-bc85-0285defa5425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "100/100 [==============================] - 21s 180ms/step - loss: 0.1308 - val_loss: 0.1073\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.1109 - val_loss: 0.0795\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 18s 180ms/step - loss: 0.0676 - val_loss: 0.0110\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0538 - val_loss: 0.0817\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0428 - val_loss: 0.0192\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.0357 - val_loss: 0.0486\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.0328 - val_loss: 0.0273\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0310 - val_loss: 0.0251\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0294 - val_loss: 0.0307\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0271 - val_loss: 0.0141\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0273 - val_loss: 0.0153\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0262 - val_loss: 0.0082\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0243 - val_loss: 0.0093\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.0235 - val_loss: 0.0078\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0242 - val_loss: 0.0156\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0236 - val_loss: 0.0147\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 19s 187ms/step - loss: 0.0227 - val_loss: 0.0288\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0228 - val_loss: 0.0265\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 18s 177ms/step - loss: 0.0227 - val_loss: 0.0173\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0223 - val_loss: 0.0097\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 18s 176ms/step - loss: 0.0216 - val_loss: 0.0156\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 18s 179ms/step - loss: 0.0216 - val_loss: 0.0241\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.0214 - val_loss: 0.0114\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.0212 - val_loss: 0.0192\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0206 - val_loss: 0.0107\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0209 - val_loss: 0.0129\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 19s 189ms/step - loss: 0.0210 - val_loss: 0.0118\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0203 - val_loss: 0.0093\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.0207 - val_loss: 0.0094\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0207 - val_loss: 0.0203\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0202 - val_loss: 0.0074\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0199 - val_loss: 0.0133\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0199 - val_loss: 0.0136\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0198 - val_loss: 0.0234\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.0200 - val_loss: 0.0099\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.0190 - val_loss: 0.0227\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 18s 179ms/step - loss: 0.0196 - val_loss: 0.0329\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.0197 - val_loss: 0.0176\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.0197 - val_loss: 0.0081\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 17s 176ms/step - loss: 0.0191 - val_loss: 0.0105\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 18s 179ms/step - loss: 0.0191 - val_loss: 0.0240\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.0193 - val_loss: 0.0174\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.0190 - val_loss: 0.0142\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 18s 176ms/step - loss: 0.0185 - val_loss: 0.0085\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 19s 196ms/step - loss: 0.0191 - val_loss: 0.0122\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 19s 195ms/step - loss: 0.0190 - val_loss: 0.0199\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.0186 - val_loss: 0.0139\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.0184 - val_loss: 0.0072\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.0184 - val_loss: 0.0137\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 20s 196ms/step - loss: 0.0184 - val_loss: 0.0084\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.0184 - val_loss: 0.0139\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 18s 177ms/step - loss: 0.0180 - val_loss: 0.0070\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 17s 176ms/step - loss: 0.0185 - val_loss: 0.0098\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.0179 - val_loss: 0.0063\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 21s 215ms/step - loss: 0.0175 - val_loss: 0.0177\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.0179 - val_loss: 0.0186\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 18s 177ms/step - loss: 0.0178 - val_loss: 0.0162\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.0185 - val_loss: 0.0065\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.0177 - val_loss: 0.0111\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.0178 - val_loss: 0.0216\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 18s 177ms/step - loss: 0.0176 - val_loss: 0.0095\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.0179 - val_loss: 0.0081\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 19s 189ms/step - loss: 0.0174 - val_loss: 0.0119\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 19s 189ms/step - loss: 0.0177 - val_loss: 0.0097\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 18s 179ms/step - loss: 0.0174 - val_loss: 0.0133\n",
            "Epoch 66/200\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.0174 - val_loss: 0.0115\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.0177 - val_loss: 0.0081\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.0178 - val_loss: 0.0144\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 18s 176ms/step - loss: 0.0175 - val_loss: 0.0143\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 18s 179ms/step - loss: 0.0170 - val_loss: 0.0064\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 17s 176ms/step - loss: 0.0172 - val_loss: 0.0077\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 17s 176ms/step - loss: 0.0173 - val_loss: 0.0071\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 19s 195ms/step - loss: 0.0175 - val_loss: 0.0068\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0171 - val_loss: 0.0152\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.0169 - val_loss: 0.0133\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 18s 177ms/step - loss: 0.0170 - val_loss: 0.0087\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0169 - val_loss: 0.0079\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0169 - val_loss: 0.0100\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.0171 - val_loss: 0.0195\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.0168 - val_loss: 0.0076\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0170 - val_loss: 0.0066\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 19s 195ms/step - loss: 0.0167 - val_loss: 0.0080\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.0168 - val_loss: 0.0073\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0168 - val_loss: 0.0093\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0168 - val_loss: 0.0127\n",
            "Epoch 86/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0167 - val_loss: 0.0096\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0167 - val_loss: 0.0109\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0167 - val_loss: 0.0190\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.0166 - val_loss: 0.0080\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0163 - val_loss: 0.0065\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.0164 - val_loss: 0.0075\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.0170 - val_loss: 0.0078\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0164 - val_loss: 0.0132\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0166 - val_loss: 0.0234\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0165 - val_loss: 0.0107\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0167 - val_loss: 0.0070\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0165 - val_loss: 0.0115\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0164 - val_loss: 0.0113\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0162 - val_loss: 0.0070\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 19s 189ms/step - loss: 0.0166 - val_loss: 0.0122\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0162 - val_loss: 0.0087\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0163 - val_loss: 0.0072\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0164 - val_loss: 0.0108\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0164 - val_loss: 0.0069\n",
            "Epoch 105/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0161 - val_loss: 0.0150\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0163 - val_loss: 0.0071\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0164 - val_loss: 0.0166\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0162 - val_loss: 0.0066\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 18s 179ms/step - loss: 0.0162 - val_loss: 0.0106\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 18s 179ms/step - loss: 0.0161 - val_loss: 0.0082\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.0164 - val_loss: 0.0068\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0159 - val_loss: 0.0165\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0160 - val_loss: 0.0229\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0161 - val_loss: 0.0079\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0164 - val_loss: 0.0067\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0161 - val_loss: 0.0081\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0164 - val_loss: 0.0116\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0156 - val_loss: 0.0067\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.0163 - val_loss: 0.0067\n",
            "Epoch 120/200\n",
            "100/100 [==============================] - 19s 191ms/step - loss: 0.0158 - val_loss: 0.0070\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0160 - val_loss: 0.0068\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 16s 165ms/step - loss: 0.0161 - val_loss: 0.0126\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0158 - val_loss: 0.0100\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0160 - val_loss: 0.0084\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0159 - val_loss: 0.0076\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0160 - val_loss: 0.0130\n",
            "Epoch 127/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0157 - val_loss: 0.0085\n",
            "Epoch 128/200\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.0159 - val_loss: 0.0082\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0158 - val_loss: 0.0111\n",
            "Epoch 130/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0160 - val_loss: 0.0075\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0156 - val_loss: 0.0163\n",
            "Epoch 132/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0157 - val_loss: 0.0216\n",
            "Epoch 133/200\n",
            "100/100 [==============================] - 16s 165ms/step - loss: 0.0155 - val_loss: 0.0078\n",
            "Epoch 134/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0160 - val_loss: 0.0066\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0158 - val_loss: 0.0092\n",
            "Epoch 136/200\n",
            "100/100 [==============================] - 16s 165ms/step - loss: 0.0155 - val_loss: 0.0074\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.0155 - val_loss: 0.0081\n",
            "Epoch 138/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0161 - val_loss: 0.0097\n",
            "Epoch 139/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0158 - val_loss: 0.0068\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0157 - val_loss: 0.0069\n",
            "Epoch 141/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0156 - val_loss: 0.0110\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0156 - val_loss: 0.0069\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0156 - val_loss: 0.0090\n",
            "Epoch 144/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0157 - val_loss: 0.0083\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0157 - val_loss: 0.0124\n",
            "Epoch 146/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0154 - val_loss: 0.0080\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.0157 - val_loss: 0.0101\n",
            "Epoch 148/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0154 - val_loss: 0.0102\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0158 - val_loss: 0.0073\n",
            "Epoch 150/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0155 - val_loss: 0.0122\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0155 - val_loss: 0.0209\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0154 - val_loss: 0.0082\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0157 - val_loss: 0.0103\n",
            "Epoch 154/200\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.0156 - val_loss: 0.0086\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.0156 - val_loss: 0.0089\n",
            "Epoch 156/200\n",
            "100/100 [==============================] - 19s 190ms/step - loss: 0.0152 - val_loss: 0.0082\n",
            "Epoch 157/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0158 - val_loss: 0.0088\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0154 - val_loss: 0.0072\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0155 - val_loss: 0.0069\n",
            "Epoch 160/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0154 - val_loss: 0.0125\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0154 - val_loss: 0.0106\n",
            "Epoch 162/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0153 - val_loss: 0.0087\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0155 - val_loss: 0.0107\n",
            "Epoch 164/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0152 - val_loss: 0.0131\n",
            "Epoch 165/200\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.0154 - val_loss: 0.0085\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0152 - val_loss: 0.0066\n",
            "Epoch 167/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0152 - val_loss: 0.0089\n",
            "Epoch 168/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0156 - val_loss: 0.0068\n",
            "Epoch 169/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0153 - val_loss: 0.0177\n",
            "Epoch 170/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0150 - val_loss: 0.0176\n",
            "Epoch 171/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0154 - val_loss: 0.0106\n",
            "Epoch 172/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0154 - val_loss: 0.0089\n",
            "Epoch 173/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0152 - val_loss: 0.0101\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.0155 - val_loss: 0.0195\n",
            "Epoch 175/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0150 - val_loss: 0.0131\n",
            "Epoch 176/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0155 - val_loss: 0.0104\n",
            "Epoch 177/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0149 - val_loss: 0.0072\n",
            "Epoch 178/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0152 - val_loss: 0.0088\n",
            "Epoch 179/200\n",
            "100/100 [==============================] - 16s 165ms/step - loss: 0.0154 - val_loss: 0.0138\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0152 - val_loss: 0.0096\n",
            "Epoch 182/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0150 - val_loss: 0.0135\n",
            "Epoch 183/200\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.0153 - val_loss: 0.0068\n",
            "Epoch 184/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0150 - val_loss: 0.0074\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0154 - val_loss: 0.0092\n",
            "Epoch 186/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0150 - val_loss: 0.0093\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0155 - val_loss: 0.0068\n",
            "Epoch 188/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0150 - val_loss: 0.0183\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0149 - val_loss: 0.0141\n",
            "Epoch 190/200\n",
            "100/100 [==============================] - 16s 165ms/step - loss: 0.0150 - val_loss: 0.0095\n",
            "Epoch 191/200\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0154 - val_loss: 0.0071\n",
            "Epoch 192/200\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.0150 - val_loss: 0.0073\n",
            "Epoch 193/200\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.0148 - val_loss: 0.0188\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 0.0149 - val_loss: 0.0068\n",
            "Epoch 195/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0149 - val_loss: 0.0068\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0150 - val_loss: 0.0069\n",
            "Epoch 197/200\n",
            "100/100 [==============================] - 16s 166ms/step - loss: 0.0148 - val_loss: 0.0068\n",
            "Epoch 198/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0149 - val_loss: 0.0089\n",
            "Epoch 199/200\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0149 - val_loss: 0.0078\n",
            "Epoch 200/200\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0148 - val_loss: 0.0092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model.evaluate(test_gen, steps = test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUzIN4Hc3eJT",
        "outputId": "e76443f2-b005-4088-fc2f-b514e38ffac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "525/525 [==============================] - 14s 27ms/step - loss: 0.0771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"drive/MyDrive/Engineer's Project/standard_gru_eur_pln_200epochs.h5\")"
      ],
      "metadata": {
        "id": "pN8EZDX9363S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "5A9W0nE9cvwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 epochs - 0.03 <br>\n",
        "100 - 0.0195 <br>\n",
        "200  - 0.0147 <br>\n",
        "300 -  0.0184 <br>\n",
        "400 - 0.0149<br>\n",
        "600 - 0.0213<br>\n",
        "all with dropout 0<br>"
      ],
      "metadata": {
        "id": "zm84sRdB40k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with correct delay = 0\n",
        "200 epok - 0.0111!"
      ],
      "metadata": {
        "id": "edABWaMbdVY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tpcL5klxEU9w",
        "outputId": "b1ad490d-f1db-4d7a-c051-cc54a9727b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "200/200 [==============================] - 18s 81ms/step - loss: 0.1990 - val_loss: 0.0854\n",
            "Epoch 2/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0827 - val_loss: 0.0254\n",
            "Epoch 3/200\n",
            "200/200 [==============================] - 16s 79ms/step - loss: 0.0630 - val_loss: 0.0461\n",
            "Epoch 4/200\n",
            "200/200 [==============================] - 16s 79ms/step - loss: 0.0537 - val_loss: 0.0221\n",
            "Epoch 5/200\n",
            "200/200 [==============================] - 16s 79ms/step - loss: 0.0473 - val_loss: 0.0191\n",
            "Epoch 6/200\n",
            "200/200 [==============================] - 16s 79ms/step - loss: 0.0453 - val_loss: 0.0718\n",
            "Epoch 7/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0420 - val_loss: 0.0517\n",
            "Epoch 8/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0404 - val_loss: 0.0497\n",
            "Epoch 9/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0391 - val_loss: 0.0184\n",
            "Epoch 10/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0382 - val_loss: 0.0438\n",
            "Epoch 11/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0375 - val_loss: 0.0743\n",
            "Epoch 12/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0368 - val_loss: 0.0547\n",
            "Epoch 13/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0372 - val_loss: 0.0698\n",
            "Epoch 14/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0363 - val_loss: 0.0541\n",
            "Epoch 15/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0361 - val_loss: 0.0350\n",
            "Epoch 16/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0349 - val_loss: 0.0340\n",
            "Epoch 17/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0353 - val_loss: 0.0435\n",
            "Epoch 18/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0340 - val_loss: 0.0469\n",
            "Epoch 19/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0342 - val_loss: 0.0410\n",
            "Epoch 20/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0333 - val_loss: 0.0224\n",
            "Epoch 21/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0335 - val_loss: 0.0447\n",
            "Epoch 22/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0331 - val_loss: 0.0366\n",
            "Epoch 23/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0331 - val_loss: 0.0413\n",
            "Epoch 24/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0321 - val_loss: 0.0332\n",
            "Epoch 25/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0324 - val_loss: 0.0479\n",
            "Epoch 26/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0320 - val_loss: 0.0202\n",
            "Epoch 27/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0318 - val_loss: 0.0460\n",
            "Epoch 28/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0317 - val_loss: 0.0280\n",
            "Epoch 29/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0313 - val_loss: 0.0479\n",
            "Epoch 30/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0313 - val_loss: 0.0547\n",
            "Epoch 31/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0310 - val_loss: 0.0633\n",
            "Epoch 32/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0310 - val_loss: 0.0512\n",
            "Epoch 33/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0302 - val_loss: 0.0548\n",
            "Epoch 34/200\n",
            "200/200 [==============================] - 16s 80ms/step - loss: 0.0304 - val_loss: 0.0405\n",
            "Epoch 35/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0301 - val_loss: 0.0456\n",
            "Epoch 36/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0299 - val_loss: 0.0419\n",
            "Epoch 37/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0292 - val_loss: 0.0371\n",
            "Epoch 38/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0293 - val_loss: 0.0412\n",
            "Epoch 39/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0289 - val_loss: 0.0237\n",
            "Epoch 40/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0293 - val_loss: 0.0604\n",
            "Epoch 41/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0284 - val_loss: 0.0112\n",
            "Epoch 42/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0288 - val_loss: 0.0535\n",
            "Epoch 43/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0280 - val_loss: 0.0374\n",
            "Epoch 44/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0281 - val_loss: 0.0590\n",
            "Epoch 45/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0277 - val_loss: 0.0110\n",
            "Epoch 46/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0273 - val_loss: 0.0632\n",
            "Epoch 47/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0271 - val_loss: 0.0577\n",
            "Epoch 48/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0265 - val_loss: 0.0699\n",
            "Epoch 49/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0260 - val_loss: 0.0861\n",
            "Epoch 50/200\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0254 - val_loss: 0.0906\n",
            "Epoch 51/200\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0251 - val_loss: 0.0869\n",
            "Epoch 52/200\n",
            "200/200 [==============================] - 17s 85ms/step - loss: 0.0250 - val_loss: 0.0843\n",
            "Epoch 53/200\n",
            "200/200 [==============================] - 16s 83ms/step - loss: 0.0245 - val_loss: 0.0662\n",
            "Epoch 54/200\n",
            "200/200 [==============================] - 17s 86ms/step - loss: 0.0246 - val_loss: 0.0534\n",
            "Epoch 55/200\n",
            "200/200 [==============================] - 17s 86ms/step - loss: 0.0246 - val_loss: 0.0689\n",
            "Epoch 56/200\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0238 - val_loss: 0.0793\n",
            "Epoch 57/200\n",
            "200/200 [==============================] - 17s 86ms/step - loss: 0.0241 - val_loss: 0.0648\n",
            "Epoch 58/200\n",
            "200/200 [==============================] - 17s 86ms/step - loss: 0.0238 - val_loss: 0.0399\n",
            "Epoch 59/200\n",
            "200/200 [==============================] - 17s 85ms/step - loss: 0.0238 - val_loss: 0.0724\n",
            "Epoch 60/200\n",
            "200/200 [==============================] - 16s 83ms/step - loss: 0.0234 - val_loss: 0.0518\n",
            "Epoch 61/200\n",
            "200/200 [==============================] - 16s 83ms/step - loss: 0.0233 - val_loss: 0.0646\n",
            "Epoch 62/200\n",
            "200/200 [==============================] - 17s 83ms/step - loss: 0.0230 - val_loss: 0.0406\n",
            "Epoch 63/200\n",
            "200/200 [==============================] - 17s 83ms/step - loss: 0.0232 - val_loss: 0.0657\n",
            "Epoch 64/200\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0229 - val_loss: 0.0294\n",
            "Epoch 65/200\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0227 - val_loss: 0.0817\n",
            "Epoch 66/200\n",
            "200/200 [==============================] - 17s 83ms/step - loss: 0.0224 - val_loss: 0.0648\n",
            "Epoch 67/200\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0226 - val_loss: 0.0674\n",
            "Epoch 68/200\n",
            "200/200 [==============================] - 17s 85ms/step - loss: 0.0222 - val_loss: 0.0862\n",
            "Epoch 69/200\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0220 - val_loss: 0.0786\n",
            "Epoch 70/200\n",
            " 50/200 [======>.......................] - ETA: 6s - loss: 0.0213"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-8b4e9e415ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   validation_steps=val_steps)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(layers.LSTM(120,\n",
        "  activation='sigmoid',\n",
        "  dropout=0.1,\n",
        "  #recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(160,\n",
        "  activation='sigmoid',))\n",
        "model.add(layers.Dense(1,\n",
        "  activation='sigmoid'))\n",
        "model.compile(optimizer= tf.keras.optimizers.RMSprop(0.01), loss='mae')\n",
        "history = model.fit(train_gen,\n",
        "  steps_per_epoch=200,\n",
        "  epochs=200,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_zXegCCEWzZ",
        "outputId": "a5e4b58b-acc6-4234-d31e-03bff14721cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "527/527 [==============================] - 12s 22ms/step - loss: 0.0130\n"
          ]
        }
      ],
      "source": [
        "eval = model.evaluate(test_gen, steps = test_steps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "VBQGCQesRy0o",
        "outputId": "2da34cd3-5a2f-40a8-dcc2-d29422d5afe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-e4293e1d3f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 900 is out of bounds for axis 0 with size 128"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save(\"drive/MyDrive/Engineer's Project/best_with_GRU2.h5\")"
      ],
      "metadata": {
        "id": "hZyoNCPK5RZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss, without CPI & Interest, CLOSING PRICE PREDICTION')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t3PJD0g7C6YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do a tuner of architecture above, 110-130, 150-170 and dropout, with step of 5"
      ],
      "metadata": {
        "id": "38OKUJHFYOCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model same, but with dropout and recurrent dropout 0.1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yjO7MqZTlMY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TmUE1wKPM-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68f136d-3414-4471-df30-8b8501a08e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "200/200 [==============================] - 22s 93ms/step - loss: 0.2084 - val_loss: 0.2017\n",
            "Epoch 2/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0775 - val_loss: 0.0804\n",
            "Epoch 3/200\n",
            "200/200 [==============================] - 18s 90ms/step - loss: 0.0611 - val_loss: 0.0450\n",
            "Epoch 4/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0529 - val_loss: 0.0238\n",
            "Epoch 5/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0478 - val_loss: 0.0629\n",
            "Epoch 6/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0462 - val_loss: 0.0333\n",
            "Epoch 7/200\n",
            "200/200 [==============================] - 18s 91ms/step - loss: 0.0428 - val_loss: 0.0220\n",
            "Epoch 8/200\n",
            "200/200 [==============================] - 19s 98ms/step - loss: 0.0420 - val_loss: 0.0426\n",
            "Epoch 9/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0400 - val_loss: 0.0541\n",
            "Epoch 10/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0397 - val_loss: 0.0415\n",
            "Epoch 11/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0384 - val_loss: 0.0268\n",
            "Epoch 12/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0387 - val_loss: 0.0678\n",
            "Epoch 13/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0376 - val_loss: 0.0255\n",
            "Epoch 14/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0368 - val_loss: 0.0406\n",
            "Epoch 15/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0365 - val_loss: 0.0164\n",
            "Epoch 16/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0358 - val_loss: 0.0598\n",
            "Epoch 17/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0358 - val_loss: 0.0619\n",
            "Epoch 18/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0350 - val_loss: 0.0674\n",
            "Epoch 19/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0349 - val_loss: 0.0517\n",
            "Epoch 20/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0346 - val_loss: 0.0321\n",
            "Epoch 21/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0344 - val_loss: 0.0170\n",
            "Epoch 22/200\n",
            "200/200 [==============================] - 21s 104ms/step - loss: 0.0336 - val_loss: 0.0257\n",
            "Epoch 23/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0336 - val_loss: 0.0333\n",
            "Epoch 24/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0330 - val_loss: 0.0421\n",
            "Epoch 25/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0333 - val_loss: 0.0292\n",
            "Epoch 26/200\n",
            "200/200 [==============================] - 23s 115ms/step - loss: 0.0325 - val_loss: 0.0155\n",
            "Epoch 27/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0325 - val_loss: 0.0484\n",
            "Epoch 28/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0318 - val_loss: 0.0280\n",
            "Epoch 29/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0322 - val_loss: 0.0470\n",
            "Epoch 30/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0312 - val_loss: 0.0209\n",
            "Epoch 31/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0314 - val_loss: 0.0515\n",
            "Epoch 32/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0309 - val_loss: 0.0269\n",
            "Epoch 33/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0311 - val_loss: 0.0453\n",
            "Epoch 34/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0304 - val_loss: 0.0292\n",
            "Epoch 35/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0300 - val_loss: 0.0475\n",
            "Epoch 36/200\n",
            "200/200 [==============================] - 20s 101ms/step - loss: 0.0308 - val_loss: 0.0716\n",
            "Epoch 37/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0302 - val_loss: 0.0587\n",
            "Epoch 38/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0298 - val_loss: 0.0659\n",
            "Epoch 39/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0293 - val_loss: 0.0622\n",
            "Epoch 40/200\n",
            "200/200 [==============================] - 18s 91ms/step - loss: 0.0296 - val_loss: 0.0266\n",
            "Epoch 41/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0291 - val_loss: 0.0424\n",
            "Epoch 42/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0291 - val_loss: 0.0384\n",
            "Epoch 43/200\n",
            "200/200 [==============================] - 18s 91ms/step - loss: 0.0283 - val_loss: 0.0530\n",
            "Epoch 44/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0285 - val_loss: 0.0486\n",
            "Epoch 45/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0283 - val_loss: 0.0385\n",
            "Epoch 46/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0286 - val_loss: 0.0517\n",
            "Epoch 47/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0278 - val_loss: 0.0143\n",
            "Epoch 48/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0277 - val_loss: 0.0565\n",
            "Epoch 49/200\n",
            "200/200 [==============================] - 21s 107ms/step - loss: 0.0274 - val_loss: 0.0374\n",
            "Epoch 50/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0275 - val_loss: 0.0632\n",
            "Epoch 51/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0271 - val_loss: 0.0122\n",
            "Epoch 52/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0272 - val_loss: 0.0623\n",
            "Epoch 53/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0268 - val_loss: 0.0502\n",
            "Epoch 54/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0268 - val_loss: 0.0681\n",
            "Epoch 55/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0266 - val_loss: 0.0801\n",
            "Epoch 56/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0262 - val_loss: 0.0749\n",
            "Epoch 57/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0263 - val_loss: 0.0822\n",
            "Epoch 58/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0259 - val_loss: 0.0761\n",
            "Epoch 59/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0259 - val_loss: 0.0521\n",
            "Epoch 60/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0251 - val_loss: 0.0511\n",
            "Epoch 61/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0247 - val_loss: 0.0617\n",
            "Epoch 62/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0247 - val_loss: 0.0623\n",
            "Epoch 63/200\n",
            "200/200 [==============================] - 21s 103ms/step - loss: 0.0244 - val_loss: 0.0565\n",
            "Epoch 64/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0243 - val_loss: 0.0299\n",
            "Epoch 65/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0244 - val_loss: 0.0683\n",
            "Epoch 66/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0236 - val_loss: 0.0411\n",
            "Epoch 67/200\n",
            "200/200 [==============================] - 18s 92ms/step - loss: 0.0237 - val_loss: 0.0622\n",
            "Epoch 68/200\n",
            "200/200 [==============================] - 23s 114ms/step - loss: 0.0234 - val_loss: 0.0566\n",
            "Epoch 69/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0233 - val_loss: 0.0637\n",
            "Epoch 70/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0227 - val_loss: 0.0223\n",
            "Epoch 71/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0228 - val_loss: 0.0698\n",
            "Epoch 72/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0225 - val_loss: 0.0536\n",
            "Epoch 73/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0221 - val_loss: 0.0726\n",
            "Epoch 74/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0221 - val_loss: 0.0860\n",
            "Epoch 75/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0217 - val_loss: 0.0759\n",
            "Epoch 76/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0215 - val_loss: 0.0790\n",
            "Epoch 77/200\n",
            "200/200 [==============================] - 20s 102ms/step - loss: 0.0211 - val_loss: 0.0725\n",
            "Epoch 78/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0212 - val_loss: 0.0554\n",
            "Epoch 79/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0209 - val_loss: 0.0611\n",
            "Epoch 80/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0207 - val_loss: 0.0585\n",
            "Epoch 81/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0208 - val_loss: 0.0743\n",
            "Epoch 82/200\n",
            "200/200 [==============================] - 18s 91ms/step - loss: 0.0204 - val_loss: 0.0598\n",
            "Epoch 83/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0201 - val_loss: 0.0472\n",
            "Epoch 84/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0204 - val_loss: 0.0713\n",
            "Epoch 85/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0200 - val_loss: 0.0661\n",
            "Epoch 86/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0201 - val_loss: 0.0600\n",
            "Epoch 87/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0197 - val_loss: 0.0339\n",
            "Epoch 88/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0198 - val_loss: 0.0731\n",
            "Epoch 89/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0194 - val_loss: 0.0486\n",
            "Epoch 90/200\n",
            "200/200 [==============================] - 23s 115ms/step - loss: 0.0195 - val_loss: 0.0668\n",
            "Epoch 91/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0193 - val_loss: 0.0672\n",
            "Epoch 92/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0193 - val_loss: 0.0786\n",
            "Epoch 93/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0191 - val_loss: 0.0901\n",
            "Epoch 94/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0190 - val_loss: 0.0784\n",
            "Epoch 95/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0188 - val_loss: 0.0858\n",
            "Epoch 96/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0189 - val_loss: 0.0789\n",
            "Epoch 97/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0184 - val_loss: 0.0639\n",
            "Epoch 98/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0186 - val_loss: 0.0543\n",
            "Epoch 99/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0185 - val_loss: 0.0654\n",
            "Epoch 100/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0187 - val_loss: 0.0778\n",
            "Epoch 101/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0184 - val_loss: 0.0638\n",
            "Epoch 102/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0182 - val_loss: 0.0488\n",
            "Epoch 103/200\n",
            "200/200 [==============================] - 21s 106ms/step - loss: 0.0184 - val_loss: 0.0609\n",
            "Epoch 104/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0184 - val_loss: 0.0593\n",
            "Epoch 105/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0182 - val_loss: 0.0691\n",
            "Epoch 106/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0181 - val_loss: 0.0633\n",
            "Epoch 107/200\n",
            "200/200 [==============================] - 19s 98ms/step - loss: 0.0181 - val_loss: 0.0764\n",
            "Epoch 108/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0179 - val_loss: 0.0417\n",
            "Epoch 109/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0181 - val_loss: 0.0745\n",
            "Epoch 110/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0177 - val_loss: 0.0532\n",
            "Epoch 111/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0181 - val_loss: 0.0765\n",
            "Epoch 112/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0178 - val_loss: 0.0910\n",
            "Epoch 113/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0178 - val_loss: 0.0754\n",
            "Epoch 114/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0176 - val_loss: 0.0822\n",
            "Epoch 115/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0177 - val_loss: 0.0722\n",
            "Epoch 116/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0178 - val_loss: 0.0676\n",
            "Epoch 117/200\n",
            "200/200 [==============================] - 21s 106ms/step - loss: 0.0175 - val_loss: 0.0615\n",
            "Epoch 118/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0176 - val_loss: 0.0656\n",
            "Epoch 119/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0178 - val_loss: 0.0790\n",
            "Epoch 120/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0177 - val_loss: 0.0719\n",
            "Epoch 121/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0175 - val_loss: 0.0507\n",
            "Epoch 122/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0178 - val_loss: 0.0722\n",
            "Epoch 123/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0174 - val_loss: 0.0768\n",
            "Epoch 124/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0176 - val_loss: 0.0686\n",
            "Epoch 125/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0173 - val_loss: 0.0421\n",
            "Epoch 126/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0175 - val_loss: 0.0719\n",
            "Epoch 127/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0173 - val_loss: 0.0609\n",
            "Epoch 128/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0174 - val_loss: 0.0653\n",
            "Epoch 129/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0173 - val_loss: 0.0717\n",
            "Epoch 130/200\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0170 - val_loss: 0.0751\n",
            "Epoch 131/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0172 - val_loss: 0.0784\n",
            "Epoch 132/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0172 - val_loss: 0.0792\n",
            "Epoch 133/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0173 - val_loss: 0.0688\n",
            "Epoch 134/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0171 - val_loss: 0.0768\n",
            "Epoch 135/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0173 - val_loss: 0.0749\n",
            "Epoch 136/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0170 - val_loss: 0.0690\n",
            "Epoch 137/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0171 - val_loss: 0.0770\n",
            "Epoch 138/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0169 - val_loss: 0.0742\n",
            "Epoch 139/200\n",
            "200/200 [==============================] - 19s 98ms/step - loss: 0.0170 - val_loss: 0.0689\n",
            "Epoch 140/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0168 - val_loss: 0.0509\n",
            "Epoch 141/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0171 - val_loss: 0.0744\n",
            "Epoch 142/200\n",
            "200/200 [==============================] - 21s 106ms/step - loss: 0.0169 - val_loss: 0.0556\n",
            "Epoch 143/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0170 - val_loss: 0.0734\n",
            "Epoch 144/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0167 - val_loss: 0.0411\n",
            "Epoch 145/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0170 - val_loss: 0.0763\n",
            "Epoch 146/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0166 - val_loss: 0.0566\n",
            "Epoch 147/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0169 - val_loss: 0.0754\n",
            "Epoch 148/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0168 - val_loss: 0.0693\n",
            "Epoch 149/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0169 - val_loss: 0.0767\n",
            "Epoch 150/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0167 - val_loss: 0.0850\n",
            "Epoch 151/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0167 - val_loss: 0.0817\n",
            "Epoch 152/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0166 - val_loss: 0.0848\n",
            "Epoch 153/200\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0166 - val_loss: 0.0824\n",
            "Epoch 154/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0167 - val_loss: 0.0751\n",
            "Epoch 155/200\n",
            "200/200 [==============================] - 21s 103ms/step - loss: 0.0167 - val_loss: 0.0686\n",
            "Epoch 156/200\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0166 - val_loss: 0.0645\n",
            "Epoch 157/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0165 - val_loss: 0.0611\n",
            "Epoch 158/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0167 - val_loss: 0.0666\n",
            "Epoch 159/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0165 - val_loss: 0.0681\n",
            "Epoch 160/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0165 - val_loss: 0.0728\n",
            "Epoch 161/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0164 - val_loss: 0.0670\n",
            "Epoch 162/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0166 - val_loss: 0.0683\n",
            "Epoch 163/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0164 - val_loss: 0.0426\n",
            "Epoch 164/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0164 - val_loss: 0.0730\n",
            "Epoch 165/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0163 - val_loss: 0.0450\n",
            "Epoch 166/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0164 - val_loss: 0.0644\n",
            "Epoch 167/200\n",
            "200/200 [==============================] - 20s 99ms/step - loss: 0.0165 - val_loss: 0.0797\n",
            "Epoch 168/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0164 - val_loss: 0.0717\n",
            "Epoch 169/200\n",
            "200/200 [==============================] - 21s 103ms/step - loss: 0.0163 - val_loss: 0.0827\n",
            "Epoch 170/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0164 - val_loss: 0.0733\n",
            "Epoch 171/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0162 - val_loss: 0.0892\n",
            "Epoch 172/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0164 - val_loss: 0.0747\n",
            "Epoch 173/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0161 - val_loss: 0.0726\n",
            "Epoch 174/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0163 - val_loss: 0.0811\n",
            "Epoch 175/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0162 - val_loss: 0.0717\n",
            "Epoch 176/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0162 - val_loss: 0.0681\n",
            "Epoch 177/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0163 - val_loss: 0.0700\n",
            "Epoch 178/200\n",
            "200/200 [==============================] - 20s 100ms/step - loss: 0.0161 - val_loss: 0.0525\n",
            "Epoch 179/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0162 - val_loss: 0.0715\n",
            "Epoch 180/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0162 - val_loss: 0.0463\n",
            "Epoch 181/200\n",
            "200/200 [==============================] - 21s 106ms/step - loss: 0.0162 - val_loss: 0.0679\n",
            "Epoch 182/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0161 - val_loss: 0.0455\n",
            "Epoch 183/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0161 - val_loss: 0.0733\n",
            "Epoch 184/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0161 - val_loss: 0.0530\n",
            "Epoch 185/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0161 - val_loss: 0.0718\n",
            "Epoch 186/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0162 - val_loss: 0.0678\n",
            "Epoch 187/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0161 - val_loss: 0.0690\n",
            "Epoch 188/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0161 - val_loss: 0.0752\n",
            "Epoch 189/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0160 - val_loss: 0.0677\n",
            "Epoch 190/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0161 - val_loss: 0.0699\n",
            "Epoch 191/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0160 - val_loss: 0.0716\n",
            "Epoch 192/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0160 - val_loss: 0.0636\n",
            "Epoch 193/200\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0162 - val_loss: 0.0679\n",
            "Epoch 194/200\n",
            "200/200 [==============================] - 21s 104ms/step - loss: 0.0160 - val_loss: 0.0757\n",
            "Epoch 195/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0160 - val_loss: 0.0732\n",
            "Epoch 196/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0161 - val_loss: 0.0698\n",
            "Epoch 197/200\n",
            "200/200 [==============================] - 19s 96ms/step - loss: 0.0159 - val_loss: 0.0567\n",
            "Epoch 198/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0161 - val_loss: 0.0781\n",
            "Epoch 199/200\n",
            "200/200 [==============================] - 20s 98ms/step - loss: 0.0159 - val_loss: 0.0705\n",
            "Epoch 200/200\n",
            "200/200 [==============================] - 19s 97ms/step - loss: 0.0159 - val_loss: 0.0657\n"
          ]
        }
      ],
      "source": [
        "model1 = tf.keras.models.Sequential()\n",
        "model1.add(layers.LSTM(120,\n",
        "  activation='sigmoid',\n",
        "  dropout=0.1,\n",
        "  recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model1.add(layers.Dense(160,\n",
        "  activation='sigmoid',))\n",
        "model1.add(layers.Dense(1,\n",
        "  activation='sigmoid'))\n",
        "model1.compile(optimizer= tf.keras.optimizers.RMSprop(0.01), loss='mae')\n",
        "history1 = model1.fit(train_gen,\n",
        "  steps_per_epoch=200,\n",
        "  epochs=200,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval1 = model1.evaluate(test_gen, steps = test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uTxtKF0K-a",
        "outputId": "0725c1e3-99aa-4f91-848e-8d54d53b41e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "527/527 [==============================] - 12s 23ms/step - loss: 0.0681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model2"
      ],
      "metadata": {
        "id": "xSMVmUH5irx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "without steps per epoch- infinity\n",
        "120-160 without dropout 0.017\n",
        "120-5 dropout 0.15"
      ],
      "metadata": {
        "id": "EkWBHx89ijRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.models.Sequential()\n",
        "model2.add(layers.LSTM(120,\n",
        "  activation='sigmoid',\n",
        "  dropout=0.15,\n",
        "  recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model2.add(layers.Dense(5,\n",
        "  activation='sigmoid',))\n",
        "model2.add(layers.Dense(1,\n",
        "  activation='sigmoid'))\n",
        "model2.compile(optimizer= tf.keras.optimizers.RMSprop(0.01), loss='mae')\n",
        "history2 = model2.fit(train_gen,\n",
        "  steps_per_epoch=50,\n",
        "  epochs=200,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTm2ga9m0yYG",
        "outputId": "06ad91df-fbeb-45a6-f488-7612d26167cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "50/50 [==============================] - 9s 144ms/step - loss: 0.1801 - val_loss: 0.0402\n",
            "Epoch 2/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.1337 - val_loss: 0.2722\n",
            "Epoch 3/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0901 - val_loss: 0.0824\n",
            "Epoch 4/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0673 - val_loss: 0.0667\n",
            "Epoch 5/200\n",
            "50/50 [==============================] - 7s 136ms/step - loss: 0.0683 - val_loss: 0.0436\n",
            "Epoch 6/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0603 - val_loss: 0.0552\n",
            "Epoch 7/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0584 - val_loss: 0.0269\n",
            "Epoch 8/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0613 - val_loss: 0.1071\n",
            "Epoch 9/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0548 - val_loss: 0.0437\n",
            "Epoch 10/200\n",
            "50/50 [==============================] - 8s 155ms/step - loss: 0.0552 - val_loss: 0.0596\n",
            "Epoch 11/200\n",
            "50/50 [==============================] - 7s 136ms/step - loss: 0.0539 - val_loss: 0.0418\n",
            "Epoch 12/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0501 - val_loss: 0.0318\n",
            "Epoch 13/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0507 - val_loss: 0.0464\n",
            "Epoch 14/200\n",
            "50/50 [==============================] - 7s 145ms/step - loss: 0.0492 - val_loss: 0.0336\n",
            "Epoch 15/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0472 - val_loss: 0.0344\n",
            "Epoch 16/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0486 - val_loss: 0.0567\n",
            "Epoch 17/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0454 - val_loss: 0.0492\n",
            "Epoch 18/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0442 - val_loss: 0.0338\n",
            "Epoch 19/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0460 - val_loss: 0.0618\n",
            "Epoch 20/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0427 - val_loss: 0.0419\n",
            "Epoch 21/200\n",
            "50/50 [==============================] - 7s 135ms/step - loss: 0.0426 - val_loss: 0.0889\n",
            "Epoch 22/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0429 - val_loss: 0.0396\n",
            "Epoch 23/200\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 0.0411 - val_loss: 0.0584\n",
            "Epoch 24/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0427 - val_loss: 0.0541\n",
            "Epoch 25/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0409 - val_loss: 0.0371\n",
            "Epoch 26/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0406 - val_loss: 0.0178\n",
            "Epoch 27/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0408 - val_loss: 0.0356\n",
            "Epoch 28/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0398 - val_loss: 0.0221\n",
            "Epoch 29/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0397 - val_loss: 0.0683\n",
            "Epoch 30/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0393 - val_loss: 0.0367\n",
            "Epoch 31/200\n",
            "50/50 [==============================] - 7s 146ms/step - loss: 0.0385 - val_loss: 0.0222\n",
            "Epoch 32/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0394 - val_loss: 0.0629\n",
            "Epoch 33/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0381 - val_loss: 0.0258\n",
            "Epoch 34/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0375 - val_loss: 0.0221\n",
            "Epoch 35/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0389 - val_loss: 0.0765\n",
            "Epoch 36/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0371 - val_loss: 0.0189\n",
            "Epoch 37/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0353 - val_loss: 0.0402\n",
            "Epoch 38/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0385 - val_loss: 0.0616\n",
            "Epoch 39/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0353 - val_loss: 0.0320\n",
            "Epoch 40/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0367 - val_loss: 0.1104\n",
            "Epoch 41/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0371 - val_loss: 0.0244\n",
            "Epoch 42/200\n",
            "50/50 [==============================] - 9s 180ms/step - loss: 0.0355 - val_loss: 0.0225\n",
            "Epoch 43/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0366 - val_loss: 0.0563\n",
            "Epoch 44/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0367 - val_loss: 0.0443\n",
            "Epoch 45/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0346 - val_loss: 0.0198\n",
            "Epoch 46/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0367 - val_loss: 0.0717\n",
            "Epoch 47/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0344 - val_loss: 0.0152\n",
            "Epoch 48/200\n",
            "50/50 [==============================] - 7s 145ms/step - loss: 0.0338 - val_loss: 0.0900\n",
            "Epoch 49/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0380 - val_loss: 0.0375\n",
            "Epoch 50/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0340 - val_loss: 0.0247\n",
            "Epoch 51/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0352 - val_loss: 0.0726\n",
            "Epoch 52/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0350 - val_loss: 0.0222\n",
            "Epoch 53/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0330 - val_loss: 0.0146\n",
            "Epoch 54/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0361 - val_loss: 0.0717\n",
            "Epoch 55/200\n",
            "50/50 [==============================] - 7s 145ms/step - loss: 0.0337 - val_loss: 0.0298\n",
            "Epoch 56/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0349 - val_loss: 0.0427\n",
            "Epoch 57/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0355 - val_loss: 0.0780\n",
            "Epoch 58/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0336 - val_loss: 0.0250\n",
            "Epoch 59/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0357 - val_loss: 0.0845\n",
            "Epoch 60/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0342 - val_loss: 0.0313\n",
            "Epoch 61/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0326 - val_loss: 0.0166\n",
            "Epoch 62/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0366 - val_loss: 0.0609\n",
            "Epoch 63/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0339 - val_loss: 0.0397\n",
            "Epoch 64/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0316 - val_loss: 0.0284\n",
            "Epoch 65/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0382 - val_loss: 0.0517\n",
            "Epoch 66/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0319 - val_loss: 0.0311\n",
            "Epoch 67/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0326 - val_loss: 0.0960\n",
            "Epoch 68/200\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 0.0345 - val_loss: 0.0908\n",
            "Epoch 69/200\n",
            "50/50 [==============================] - 7s 136ms/step - loss: 0.0319 - val_loss: 0.0220\n",
            "Epoch 70/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0348 - val_loss: 0.1074\n",
            "Epoch 71/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0334 - val_loss: 0.0225\n",
            "Epoch 72/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0330 - val_loss: 0.0166\n",
            "Epoch 73/200\n",
            "50/50 [==============================] - 7s 145ms/step - loss: 0.0331 - val_loss: 0.0787\n",
            "Epoch 74/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0329 - val_loss: 0.0445\n",
            "Epoch 75/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0321 - val_loss: 0.0513\n",
            "Epoch 76/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0403 - val_loss: 0.0891\n",
            "Epoch 77/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0317 - val_loss: 0.0179\n",
            "Epoch 78/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0323 - val_loss: 0.0814\n",
            "Epoch 79/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0355 - val_loss: 0.0395\n",
            "Epoch 80/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0314 - val_loss: 0.0165\n",
            "Epoch 81/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0338 - val_loss: 0.0784\n",
            "Epoch 82/200\n",
            "50/50 [==============================] - 8s 164ms/step - loss: 0.0320 - val_loss: 0.0715\n",
            "Epoch 83/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0326 - val_loss: 0.0500\n",
            "Epoch 84/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0382 - val_loss: 0.0651\n",
            "Epoch 85/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0309 - val_loss: 0.0426\n",
            "Epoch 86/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0305 - val_loss: 0.0877\n",
            "Epoch 87/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0328 - val_loss: 0.0535\n",
            "Epoch 88/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0338 - val_loss: 0.0336\n",
            "Epoch 89/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0327 - val_loss: 0.0941\n",
            "Epoch 90/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0339 - val_loss: 0.0302\n",
            "Epoch 91/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0306 - val_loss: 0.0292\n",
            "Epoch 92/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0340 - val_loss: 0.0828\n",
            "Epoch 93/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0303 - val_loss: 0.1210\n",
            "Epoch 94/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0318 - val_loss: 0.0505\n",
            "Epoch 95/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0328 - val_loss: 0.0637\n",
            "Epoch 96/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0319 - val_loss: 0.0151\n",
            "Epoch 97/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0313 - val_loss: 0.0965\n",
            "Epoch 98/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0326 - val_loss: 0.0475\n",
            "Epoch 99/200\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 0.0303 - val_loss: 0.0261\n",
            "Epoch 100/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0325 - val_loss: 0.0811\n",
            "Epoch 101/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0295 - val_loss: 0.0674\n",
            "Epoch 102/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0317 - val_loss: 0.0650\n",
            "Epoch 103/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0339 - val_loss: 0.0636\n",
            "Epoch 104/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0309 - val_loss: 0.0438\n",
            "Epoch 105/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0302 - val_loss: 0.1044\n",
            "Epoch 106/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0318 - val_loss: 0.0449\n",
            "Epoch 107/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0292 - val_loss: 0.0466\n",
            "Epoch 108/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0316 - val_loss: 0.0907\n",
            "Epoch 109/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0302 - val_loss: 0.0398\n",
            "Epoch 110/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0307 - val_loss: 0.0221\n",
            "Epoch 111/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0314 - val_loss: 0.0806\n",
            "Epoch 112/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0297 - val_loss: 0.1198\n",
            "Epoch 113/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0300 - val_loss: 0.0552\n",
            "Epoch 114/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0311 - val_loss: 0.0707\n",
            "Epoch 115/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0288 - val_loss: 0.0154\n",
            "Epoch 116/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0300 - val_loss: 0.1100\n",
            "Epoch 117/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0292 - val_loss: 0.0499\n",
            "Epoch 118/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0285 - val_loss: 0.0324\n",
            "Epoch 119/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0298 - val_loss: 0.0850\n",
            "Epoch 120/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0285 - val_loss: 0.0974\n",
            "Epoch 121/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0282 - val_loss: 0.0473\n",
            "Epoch 122/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0302 - val_loss: 0.0746\n",
            "Epoch 123/200\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 0.0287 - val_loss: 0.0405\n",
            "Epoch 124/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0282 - val_loss: 0.1012\n",
            "Epoch 125/200\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 0.0286 - val_loss: 0.0395\n",
            "Epoch 126/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0272 - val_loss: 0.0616\n",
            "Epoch 127/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0289 - val_loss: 0.1144\n",
            "Epoch 128/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0281 - val_loss: 0.0508\n",
            "Epoch 129/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0271 - val_loss: 0.0262\n",
            "Epoch 130/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0282 - val_loss: 0.0894\n",
            "Epoch 131/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0265 - val_loss: 0.1271\n",
            "Epoch 132/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0278 - val_loss: 0.0688\n",
            "Epoch 133/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0278 - val_loss: 0.0841\n",
            "Epoch 134/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0267 - val_loss: 0.0242\n",
            "Epoch 135/200\n",
            "50/50 [==============================] - 7s 145ms/step - loss: 0.0273 - val_loss: 0.1095\n",
            "Epoch 136/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0268 - val_loss: 0.0668\n",
            "Epoch 137/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0267 - val_loss: 0.0528\n",
            "Epoch 138/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0268 - val_loss: 0.0997\n",
            "Epoch 139/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0268 - val_loss: 0.0999\n",
            "Epoch 140/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0267 - val_loss: 0.0666\n",
            "Epoch 141/200\n",
            "50/50 [==============================] - 7s 146ms/step - loss: 0.0276 - val_loss: 0.0922\n",
            "Epoch 142/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0262 - val_loss: 0.0896\n",
            "Epoch 143/200\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 0.0262 - val_loss: 0.1127\n",
            "Epoch 144/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0261 - val_loss: 0.0762\n",
            "Epoch 145/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0262 - val_loss: 0.0687\n",
            "Epoch 146/200\n",
            "50/50 [==============================] - 7s 146ms/step - loss: 0.0265 - val_loss: 0.1028\n",
            "Epoch 147/200\n",
            "50/50 [==============================] - 7s 149ms/step - loss: 0.0261 - val_loss: 0.0747\n",
            "Epoch 148/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0243 - val_loss: 0.0652\n",
            "Epoch 149/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0267 - val_loss: 0.1153\n",
            "Epoch 150/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0253 - val_loss: 0.1285\n",
            "Epoch 151/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0255 - val_loss: 0.0900\n",
            "Epoch 152/200\n",
            "50/50 [==============================] - 7s 145ms/step - loss: 0.0262 - val_loss: 0.1048\n",
            "Epoch 153/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0252 - val_loss: 0.0605\n",
            "Epoch 154/200\n",
            "50/50 [==============================] - 7s 147ms/step - loss: 0.0252 - val_loss: 0.1268\n",
            "Epoch 155/200\n",
            "50/50 [==============================] - 8s 161ms/step - loss: 0.0242 - val_loss: 0.0762\n",
            "Epoch 156/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0248 - val_loss: 0.0678\n",
            "Epoch 157/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0247 - val_loss: 0.1081\n",
            "Epoch 158/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0245 - val_loss: 0.1110\n",
            "Epoch 159/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0235 - val_loss: 0.0942\n",
            "Epoch 160/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0253 - val_loss: 0.1084\n",
            "Epoch 161/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0240 - val_loss: 0.0935\n",
            "Epoch 162/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0240 - val_loss: 0.1282\n",
            "Epoch 163/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0245 - val_loss: 0.0815\n",
            "Epoch 164/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0240 - val_loss: 0.0651\n",
            "Epoch 165/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0250 - val_loss: 0.1196\n",
            "Epoch 166/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0236 - val_loss: 0.0835\n",
            "Epoch 167/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0242 - val_loss: 0.0580\n",
            "Epoch 168/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0247 - val_loss: 0.1183\n",
            "Epoch 169/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0230 - val_loss: 0.1422\n",
            "Epoch 170/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0237 - val_loss: 0.0871\n",
            "Epoch 171/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0240 - val_loss: 0.1133\n",
            "Epoch 172/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0229 - val_loss: 0.0529\n",
            "Epoch 173/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0234 - val_loss: 0.1297\n",
            "Epoch 174/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0233 - val_loss: 0.1046\n",
            "Epoch 175/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0233 - val_loss: 0.0821\n",
            "Epoch 176/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0231 - val_loss: 0.1068\n",
            "Epoch 177/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0223 - val_loss: 0.1155\n",
            "Epoch 178/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0226 - val_loss: 0.1107\n",
            "Epoch 179/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0235 - val_loss: 0.1019\n",
            "Epoch 180/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0241 - val_loss: 0.0972\n",
            "Epoch 181/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0224 - val_loss: 0.1243\n",
            "Epoch 182/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0226 - val_loss: 0.1014\n",
            "Epoch 183/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0225 - val_loss: 0.0666\n",
            "Epoch 184/200\n",
            "50/50 [==============================] - 7s 146ms/step - loss: 0.0237 - val_loss: 0.1347\n",
            "Epoch 185/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0218 - val_loss: 0.0750\n",
            "Epoch 186/200\n",
            "50/50 [==============================] - 7s 146ms/step - loss: 0.0224 - val_loss: 0.0638\n",
            "Epoch 187/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0227 - val_loss: 0.1151\n",
            "Epoch 188/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0220 - val_loss: 0.1343\n",
            "Epoch 189/200\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.0217 - val_loss: 0.0993\n",
            "Epoch 190/200\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 0.0229 - val_loss: 0.1104\n",
            "Epoch 191/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0224 - val_loss: 0.0608\n",
            "Epoch 192/200\n",
            "50/50 [==============================] - 7s 143ms/step - loss: 0.0220 - val_loss: 0.1394\n",
            "Epoch 193/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0221 - val_loss: 0.1146\n",
            "Epoch 194/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0224 - val_loss: 0.0860\n",
            "Epoch 195/200\n",
            "50/50 [==============================] - 7s 144ms/step - loss: 0.0217 - val_loss: 0.1129\n",
            "Epoch 196/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0214 - val_loss: 0.1102\n",
            "Epoch 197/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0215 - val_loss: 0.0876\n",
            "Epoch 198/200\n",
            "50/50 [==============================] - 7s 142ms/step - loss: 0.0224 - val_loss: 0.1145\n",
            "Epoch 199/200\n",
            "50/50 [==============================] - 7s 141ms/step - loss: 0.0215 - val_loss: 0.0967\n",
            "Epoch 200/200\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.0214 - val_loss: 0.1304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try smooting and plots"
      ],
      "metadata": {
        "id": "mkKgMX1YmXT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "smoothing_window_size = 100\n",
        "for di in range (0,3000,smoothing_window_size):\n",
        "  scaler.fit(float_data[di:di+smoothing_window_size,:])\n",
        "  float_data[di:di+smoothing_window_size,:] = scaler.transform(float_data[di:di+smoothing_window_size,:])\n",
        "scaler.fit(float_data[di+smoothing_window_size:,:])\n",
        "float_data[di+smoothing_window_size:,:] = scaler.tranform(float_data[di+smoothing_window_size:,:])\n",
        "#test data = scaler transform(float_data[3001:] transform test data"
      ],
      "metadata": {
        "id": "JtIzrPRhk1iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval2 = model2.evaluate(test_gen, steps = test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq26G-vVDrBM",
        "outputId": "3b1490bb-9e7f-4cfc-fa27-86cd80cdf8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "527/527 [==============================] - 6s 11ms/step - loss: 0.0174\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}