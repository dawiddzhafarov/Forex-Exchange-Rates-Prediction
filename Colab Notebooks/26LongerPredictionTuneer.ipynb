{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uqvufGBFnzo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from itertools import chain\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#import keras_tuner as kt\n",
        "df = pd.read_csv(\"drive/MyDrive/Engineer's Project/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWjl0cq5kM-k",
        "outputId": "b3dc164c-6c91-46aa-d3e6-9d980fc7dfeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 61 kB 33.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 71 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 81 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 92 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 102 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 122 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 133 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133 kB 28.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "wc1eMhAf2BVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.pop('usa_cpi')\n",
        "df.pop('pol_cpi')\n",
        "df.pop('usa_inter')\n",
        "df.pop('pol_inter')\n",
        "df.pop('Date')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS502CjzFpao",
        "outputId": "78375520-d079-430c-ea8f-4690b446a762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2010.11.15\n",
              "1       2010.11.16\n",
              "2       2010.11.17\n",
              "3       2010.11.18\n",
              "4       2010.11.19\n",
              "           ...    \n",
              "3537    2022.03.27\n",
              "3538    2022.03.28\n",
              "3539    2022.03.29\n",
              "3540    2022.03.30\n",
              "3541    2022.03.31\n",
              "Name: Date, Length: 3542, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_numpy = df.to_numpy() \n",
        "#len(df_numpy[:2500])\n",
        "scaler1 = scaler.fit(df_numpy[:3000])\n",
        "df_scalled = scaler1.transform(df_numpy)\n",
        "#df_scaled_all = scaler.\n",
        "df_scalled = pd.DataFrame(df_scalled, columns=[\n",
        "  'Opening', 'High', 'Low', 'Closing','Momentum', 'Range', 'ohlc'])"
      ],
      "metadata": {
        "id": "WjNdiSHvFrav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback = 30\n",
        "step = 1\n",
        "delay = 30\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "gAvZYxhrFsfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "float_data = np.array(df_scalled).astype('float32')"
      ],
      "metadata": {
        "id": "ysrqOl0zuEU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,shuffle=False, batch_size=128, step=1):\n",
        "  if max_index is None:\n",
        "    max_index = len(data) - delay - 1\n",
        "  i = min_index + lookback\n",
        "  while 1:\n",
        "    if shuffle:\n",
        "      rows = np.random.randint(\n",
        "        min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "      if i + batch_size >= max_index:\n",
        "        i = min_index + lookback\n",
        "      rows = np.arange(i, min(i + batch_size, max_index))\n",
        "      i += len(rows)\n",
        "    samples = np.zeros((len(rows),lookback // step,data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      indices = range(rows[j] - lookback, rows[j], step)\n",
        "      samples[j] = data[indices]\n",
        "      targets[j] = data[rows[j] + delay][3] \n",
        "    yield samples, targets"
      ],
      "metadata": {
        "id": "gb3c6vLLFucu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=0,\n",
        "max_index=2500 - delay,\n",
        "#shuffle=True,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "UH5cZe3aFvc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=2500-delay,\n",
        "max_index=3000 - delay,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "jigLWVSgFwUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=3000-delay,\n",
        "max_index=3542-delay,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "uQHe5fQrFxPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_steps = (3000-delay) - (2500-delay) - lookback\n",
        "test_steps = (3542 - delay) - (300 - delay) - lookback"
      ],
      "metadata": {
        "id": "_ZToRnw0FyKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(layers.LSTM(240,\n",
        "  activation='sigmoid',\n",
        "  dropout=0.15,\n",
        "  recurrent_dropout=0.15,\n",
        "  input_shape=(None, float_train_data.shape[-1])))\n",
        "model.add(layers.Dense(200,\n",
        "  activation='sigmoid',))\n",
        "model.add(layers.Dense(1,\n",
        "  activation='sigmoid'))\n",
        "model.compile(optimizer= tf.keras.optimizers.RMSprop(0.01), loss='mae')\n",
        "history = model.fit(train_gen,\n",
        "  steps_per_epoch=200,\n",
        "  epochs=50,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "YMH209BsFzDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6600ebe-b391-46f2-e77a-5a1320bf9cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "200/200 [==============================] - 53s 253ms/step - loss: 0.1123 - val_loss: 0.1412\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 53s 267ms/step - loss: 0.0863 - val_loss: 0.1177\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0899\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0877 - val_loss: 0.1409\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0863 - val_loss: 0.1175\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0877 - val_loss: 0.1408\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0863 - val_loss: 0.1179\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0866 - val_loss: 0.0897\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0877 - val_loss: 0.1412\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0863 - val_loss: 0.1178\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0866 - val_loss: 0.0899\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0877 - val_loss: 0.1410\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0863 - val_loss: 0.1176\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0877 - val_loss: 0.1409\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0863 - val_loss: 0.1175\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0877 - val_loss: 0.1407\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0863 - val_loss: 0.1178\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0866 - val_loss: 0.0897\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0877 - val_loss: 0.1411\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0863 - val_loss: 0.1177\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0866 - val_loss: 0.0899\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0877 - val_loss: 0.1410\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0863 - val_loss: 0.1176\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0877 - val_loss: 0.1408\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0863 - val_loss: 0.1179\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0866 - val_loss: 0.0897\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 52s 262ms/step - loss: 0.0877 - val_loss: 0.1412\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 50s 253ms/step - loss: 0.0863 - val_loss: 0.1178\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0899\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0877 - val_loss: 0.1411\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0863 - val_loss: 0.1177\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0877 - val_loss: 0.1409\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0863 - val_loss: 0.1175\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0877 - val_loss: 0.1408\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0863 - val_loss: 0.1179\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0877 - val_loss: 0.1409\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 50s 253ms/step - loss: 0.0863 - val_loss: 0.1175\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0898\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0877 - val_loss: 0.1407\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0863 - val_loss: 0.1179\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0866 - val_loss: 0.0897\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0877 - val_loss: 0.1411\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0863 - val_loss: 0.1177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model.evaluate(test_gen, steps = test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80mbAC7Fj9rr",
        "outputId": "2883f14f-95f9-4708-d08d-e25dd26feccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512/512 [==============================] - 25s 49ms/step - loss: 0.0550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "vnLnO2_uCwlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  #hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e5])\n",
        "  hp_units = hp.Choice('units LSTM', values=[10,40,80,120])\n",
        "  #hp_units2 = hp.Int('units Dense', min_value=150, max_value=180,step=5)\n",
        "  hp_units2 = hp.Choice('units Dense', values = [10,40,80])\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(layers.LSTM(units=hp_units,\n",
        "    #activation=hp.Choice('lstm_activation2',values=['relu', 'sigmoid','softmax']),\n",
        "    activation='sigmoid',\n",
        "    recurrent_dropout=0.1,\n",
        "    #dropout=0.5,\n",
        "    input_shape=(None, float_data.shape[-1])))\n",
        "  model.add(layers.Dropout(hp.Float('Dropout rate',min_value=0,max_value=0.5,step=0.1)))\n",
        "  model.add(layers.Dense(units=hp_units2,\n",
        "    #activation=hp.Choice('dense_activation',values=['relu', 'sigmoid','softmax'])))\n",
        "    activation='sigmoid'))\n",
        "  model.add(layers.Dense(1,\n",
        "    ##activation = hp.Choice('last_activ',values=['relu', 'sigmoid','softmax'])))\n",
        "    activation='sigmoid'))\n",
        "  model.compile(optimizer= tf.keras.optimizers.RMSprop(learning_rate=0.01), loss='mae')\n",
        "  return model"
      ],
      "metadata": {
        "id": "3P3_7mZlkCht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.BayesianOptimization(model_builder,\n",
        "    objective='val_loss',\n",
        "    max_trials=20,\n",
        "    directory = 'try4')"
      ],
      "metadata": {
        "id": "_qOh5fFFkIFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best 240 - 200 0.4 dropout 0.1 rec dropout"
      ],
      "metadata": {
        "id": "G63nVbYjVRUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(train_gen, \n",
        "    validation_data = val_gen,\n",
        "    validation_steps = val_steps, \n",
        "    epochs=10,\n",
        "    steps_per_epoch = 200,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "DCWafAf8kJ3N",
        "outputId": "60d92369-b19a-453a-a25e-725b531fc09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 05m 25s]\n",
            "val_loss: 0.035131532698869705\n",
            "\n",
            "Best val_loss So Far: 0.03444389998912811\n",
            "Total elapsed time: 00h 23m 15s\n",
            "\n",
            "Search: Running Trial #6\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "120               |120               |units LSTM\n",
            "80                |80                |units Dense\n",
            "0.4               |0.2               |Dropout rate\n",
            "\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 34s 159ms/step - loss: 0.1429 - val_loss: 0.2883\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 32s 158ms/step - loss: 0.0886 - val_loss: 0.1259\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.0810"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a9d4185d913f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         return tuner_utils.convert_to_metrics_dict(\n\u001b[1;32m    224\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperModel.fit()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1429\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1432\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new test required, since this is not using the data"
      ],
      "metadata": {
        "id": "m7q2lFtGRMnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(float_game_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZakESFxKUKU",
        "outputId": "adc7d98d-37e0-4693-9d25-cc819f7522b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "542"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_picks = 0\n",
        "value = 0\n",
        "for i in range(542-30): #len(float_train_data - lookback - delay\n",
        "  if len(float_game_data) <= 30:\n",
        "    break\n",
        "  last = float_train_data[-lookback:]\n",
        "  last = last.reshape(1,30,7)\n",
        "  prediction = model.predict(last)\n",
        "  last_val = last[0][lookback-1][3]\n",
        "  actual_val = float_game_data[30][3]\n",
        "  if((prediction > last_val) and (actual_val > last_val)):\n",
        "    correct_picks+=1\n",
        "    value += 1000 * (actual_val - last_val)\n",
        "  elif((prediction < last_val) and (actual_val < last_val)):\n",
        "    correct_picks+=1\n",
        "    value += 1000 * (last_val - actual_val)\n",
        "  else:\n",
        "    value -= 1000 * abs(actual_val - last_val)\n",
        "  float_train_data = np.vstack((float_train_data, float_game_data[0]))\n",
        "  float_game_data = np.delete(float_game_data, 0, 0)"
      ],
      "metadata": {
        "id": "zhKV_vhVQTrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_picks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPy38h7MRhhU",
        "outputId": "a950d920-4937-48bb-8964-eb3854bf27cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6oMfj5RRmqU",
        "outputId": "38ed9aad-b7da-4fa1-f7fd-13e0a2ff263d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3776.523530483246"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "version 2 updates the model"
      ],
      "metadata": {
        "id": "q2JVdmc-oJQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(layers.LSTM(180,\n",
        "    activation='sigmoid',\n",
        "    dropout=0.15,\n",
        "    recurrent_dropout=0.15,\n",
        "    input_shape=(None, float_train_data.shape[-1])))\n",
        "  model.add(layers.Dense(100,\n",
        "    activation='sigmoid',))\n",
        "  model.add(layers.Dense(1,\n",
        "    activation='sigmoid'))\n",
        "  model.compile(optimizer= tf.keras.optimizers.RMSprop(0.01), loss='mae')\n",
        "  return model"
      ],
      "metadata": {
        "id": "CnSFTESgoUf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(float_train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE3puJfNSTy5",
        "outputId": "40d0c739-a4f0-4ec9-cff0-61685e222e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "931"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_picks = 0\n",
        "value = 0\n",
        "for i in range(0,542-30):\n",
        "  if len(float_game_data) <= 30:\n",
        "    break\n",
        "  last = float_train_data[-lookback:]\n",
        "  last = last.reshape(1,30,7)\n",
        "  prediction = model.predict(last)\n",
        "  last_val = last[0][lookback-1][3]\n",
        "  actual_val = float_game_data[29][3]\n",
        "  if((prediction > last_val) and (actual_val > last_val)):\n",
        "    correct_picks+=1\n",
        "    value += 1000 * (actual_val - last_val)\n",
        "  elif((prediction < last_val) and (actual_val < last_val)):\n",
        "    correct_picks+=1\n",
        "    value += 1000 * (last_val - actual_val)\n",
        "  else:\n",
        "    value -= 1000 * abs(actual_val - last_val)\n",
        "  float_train_data = np.vstack((float_train_data, float_game_data[0]))\n",
        "  float_game_data = np.delete(float_game_data, 0, 0)\n",
        "\n",
        "  if i % 30 == 0:\n",
        "    del model\n",
        "    del train_gen\n",
        "    model = return_model()\n",
        "    train_gen = generator(float_train_data,\n",
        "      lookback=lookback,\n",
        "      delay=delay,\n",
        "      min_index=0,\n",
        "      max_index=len(float_train_data) - delay - 1,\n",
        "      #shuffle=True,\n",
        "      step=step,\n",
        "      batch_size=batch_size)\n",
        "    model.fit(train_gen,\n",
        "      steps_per_epoch=200,\n",
        "      epochs=50,\n",
        "      validation_data=val_gen,\n",
        "      validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "VXxKSwYdoIa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844aa294-adf5-4f13-ea33-5bad24b22345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "200/200 [==============================] - 56s 254ms/step - loss: 0.1143 - val_loss: 0.2022\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0844 - val_loss: 0.1014\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0701 - val_loss: 0.0715\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0624 - val_loss: 0.0972\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0584 - val_loss: 0.0880\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0556 - val_loss: 0.0782\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0548 - val_loss: 0.0791\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0534 - val_loss: 0.0845\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0524 - val_loss: 0.0759\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0523 - val_loss: 0.0774\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0512 - val_loss: 0.0799\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 50s 253ms/step - loss: 0.0512 - val_loss: 0.0703\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0505 - val_loss: 0.0770\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0494 - val_loss: 0.0812\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0492 - val_loss: 0.0726\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0491 - val_loss: 0.0785\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0488 - val_loss: 0.0931\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0484 - val_loss: 0.0784\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0483 - val_loss: 0.0771\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0485 - val_loss: 0.0812\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0474 - val_loss: 0.0733\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0478 - val_loss: 0.0757\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0466 - val_loss: 0.0797\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0468 - val_loss: 0.0755\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0466 - val_loss: 0.0770\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0459 - val_loss: 0.0813\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0452 - val_loss: 0.0764\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0445 - val_loss: 0.0779\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0436 - val_loss: 0.0800\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0434 - val_loss: 0.0741\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0430 - val_loss: 0.0840\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0425 - val_loss: 0.0818\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0425 - val_loss: 0.0792\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0422 - val_loss: 0.0809\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0418 - val_loss: 0.0977\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0418 - val_loss: 0.0776\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0415 - val_loss: 0.0789\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0411 - val_loss: 0.0827\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0409 - val_loss: 0.0772\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0409 - val_loss: 0.0839\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0401 - val_loss: 0.0847\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0402 - val_loss: 0.0745\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0402 - val_loss: 0.0804\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0402 - val_loss: 0.0896\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0396 - val_loss: 0.0748\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0395 - val_loss: 0.0801\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0392 - val_loss: 0.0882\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0399 - val_loss: 0.0760\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0390 - val_loss: 0.0770\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0393 - val_loss: 0.0889\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 53s 252ms/step - loss: 0.1064 - val_loss: 0.1733\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0803 - val_loss: 0.0852\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0663 - val_loss: 0.0757\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0607 - val_loss: 0.0948\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0573 - val_loss: 0.0993\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0572 - val_loss: 0.0751\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0562 - val_loss: 0.0860\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0545 - val_loss: 0.0870\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0537 - val_loss: 0.0760\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0534 - val_loss: 0.0800\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0520 - val_loss: 0.0896\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0514 - val_loss: 0.0749\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0508 - val_loss: 0.0791\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0496 - val_loss: 0.0828\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0496 - val_loss: 0.0716\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0495 - val_loss: 0.0717\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0485 - val_loss: 0.0791\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0485 - val_loss: 0.0737\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0488 - val_loss: 0.0813\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0478 - val_loss: 0.0801\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 50s 253ms/step - loss: 0.0483 - val_loss: 0.0787\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0476 - val_loss: 0.0728\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0470 - val_loss: 0.0879\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0466 - val_loss: 0.0766\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0467 - val_loss: 0.0791\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0463 - val_loss: 0.0850\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0469 - val_loss: 0.0786\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0459 - val_loss: 0.0826\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0456 - val_loss: 0.0821\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0461 - val_loss: 0.0739\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0453 - val_loss: 0.0763\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0452 - val_loss: 0.0822\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0448 - val_loss: 0.0784\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0450 - val_loss: 0.0803\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0443 - val_loss: 0.0850\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0440 - val_loss: 0.0785\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0439 - val_loss: 0.0845\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0432 - val_loss: 0.0946\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0430 - val_loss: 0.0796\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0430 - val_loss: 0.0830\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0420 - val_loss: 0.1000\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0416 - val_loss: 0.0802\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0413 - val_loss: 0.0842\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0414 - val_loss: 0.1023\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 50s 253ms/step - loss: 0.0413 - val_loss: 0.0794\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0407 - val_loss: 0.0832\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0406 - val_loss: 0.1167\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0404 - val_loss: 0.0862\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0399 - val_loss: 0.0808\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0400 - val_loss: 0.0987\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 53s 254ms/step - loss: 0.1329 - val_loss: 0.0892\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0810 - val_loss: 0.1299\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0679 - val_loss: 0.0649\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0651 - val_loss: 0.0797\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0606 - val_loss: 0.0556\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0602 - val_loss: 0.0752\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0589 - val_loss: 0.0714\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0581 - val_loss: 0.0877\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0584 - val_loss: 0.0713\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 50s 250ms/step - loss: 0.0564 - val_loss: 0.0669\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0571 - val_loss: 0.1016\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0556 - val_loss: 0.0691\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0556 - val_loss: 0.0853\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0545 - val_loss: 0.0885\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0537 - val_loss: 0.0762\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0533 - val_loss: 0.0851\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0522 - val_loss: 0.0786\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0524 - val_loss: 0.0782\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0509 - val_loss: 0.0771\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0514 - val_loss: 0.0946\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0503 - val_loss: 0.0824\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0498 - val_loss: 0.0975\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 50s 251ms/step - loss: 0.0491 - val_loss: 0.1161\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0489 - val_loss: 0.0809\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0489 - val_loss: 0.0933\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0472 - val_loss: 0.0906\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0468 - val_loss: 0.1326\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0623 - val_loss: 0.0640\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0546 - val_loss: 0.0651\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0505 - val_loss: 0.1041\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0457 - val_loss: 0.0855\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0451 - val_loss: 0.0845\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0438 - val_loss: 0.0861\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0432 - val_loss: 0.1189\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0433 - val_loss: 0.0886\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0430 - val_loss: 0.0922\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0444 - val_loss: 0.0868\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0467 - val_loss: 0.0618\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0444 - val_loss: 0.0862\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0424 - val_loss: 0.0815\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 50s 253ms/step - loss: 0.0429 - val_loss: 0.0953\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 50s 253ms/step - loss: 0.0422 - val_loss: 0.0690\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 50s 252ms/step - loss: 0.0440 - val_loss: 0.0775\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0415 - val_loss: 0.0902\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0413 - val_loss: 0.0761\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0409 - val_loss: 0.0890\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0405 - val_loss: 0.0728\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0405 - val_loss: 0.1179\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0404 - val_loss: 0.1062\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0401 - val_loss: 0.0876\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 53s 254ms/step - loss: 0.1595 - val_loss: 0.0943\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0846 - val_loss: 0.1183\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0689 - val_loss: 0.0660\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0659 - val_loss: 0.0697\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 52s 263ms/step - loss: 0.0622 - val_loss: 0.0556\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0609 - val_loss: 0.1041\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0594 - val_loss: 0.0695\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0589 - val_loss: 0.0638\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0586 - val_loss: 0.0669\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0573 - val_loss: 0.0814\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0574 - val_loss: 0.0624\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0565 - val_loss: 0.0670\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0571 - val_loss: 0.0819\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0549 - val_loss: 0.0714\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0550 - val_loss: 0.0761\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0583 - val_loss: 0.0722\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0556 - val_loss: 0.0797\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0611 - val_loss: 0.0806\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0518 - val_loss: 0.0713\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0510 - val_loss: 0.1052\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0503 - val_loss: 0.0754\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0499 - val_loss: 0.0881\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0493 - val_loss: 0.1018\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0485 - val_loss: 0.0868\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0487 - val_loss: 0.0903\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0497 - val_loss: 0.0797\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0481 - val_loss: 0.0986\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0474 - val_loss: 0.0880\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0465 - val_loss: 0.0880\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0477 - val_loss: 0.0888\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0460 - val_loss: 0.0759\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0455 - val_loss: 0.0970\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0442 - val_loss: 0.0752\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0437 - val_loss: 0.0875\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0430 - val_loss: 0.0709\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0425 - val_loss: 0.0824\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 53s 262ms/step - loss: 0.0418 - val_loss: 0.0918\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0413 - val_loss: 0.0742\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0419 - val_loss: 0.0838\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0407 - val_loss: 0.0800\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0407 - val_loss: 0.0978\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0408 - val_loss: 0.1169\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0404 - val_loss: 0.0829\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0397 - val_loss: 0.1094\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0393 - val_loss: 0.0860\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0406 - val_loss: 0.0838\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 52s 262ms/step - loss: 0.0398 - val_loss: 0.0777\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0396 - val_loss: 0.1084\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0390 - val_loss: 0.1163\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0388 - val_loss: 0.0917\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 53s 255ms/step - loss: 0.1319 - val_loss: 0.0905\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0831 - val_loss: 0.1130\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0689 - val_loss: 0.0659\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0656 - val_loss: 0.0713\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0622 - val_loss: 0.0694\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0614 - val_loss: 0.0987\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0594 - val_loss: 0.0930\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 52s 262ms/step - loss: 0.0593 - val_loss: 0.0650\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0588 - val_loss: 0.1044\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0576 - val_loss: 0.0768\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0576 - val_loss: 0.0915\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0560 - val_loss: 0.0801\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0555 - val_loss: 0.1066\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0541 - val_loss: 0.0890\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0534 - val_loss: 0.0957\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0555 - val_loss: 0.0908\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0542 - val_loss: 0.0689\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0569 - val_loss: 0.0732\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0538 - val_loss: 0.0718\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0540 - val_loss: 0.1148\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0537 - val_loss: 0.0851\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0523 - val_loss: 0.0904\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0514 - val_loss: 0.0665\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0504 - val_loss: 0.0828\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0502 - val_loss: 0.0878\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0492 - val_loss: 0.0665\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0482 - val_loss: 0.0729\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0471 - val_loss: 0.0783\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 52s 262ms/step - loss: 0.0461 - val_loss: 0.0803\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0456 - val_loss: 0.0993\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0450 - val_loss: 0.0878\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0444 - val_loss: 0.1064\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0438 - val_loss: 0.0836\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0436 - val_loss: 0.1002\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0426 - val_loss: 0.0909\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0422 - val_loss: 0.0912\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0416 - val_loss: 0.0852\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0410 - val_loss: 0.0832\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0414 - val_loss: 0.1134\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0404 - val_loss: 0.0810\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0405 - val_loss: 0.1247\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0399 - val_loss: 0.0793\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0410 - val_loss: 0.1080\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0403 - val_loss: 0.1019\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0395 - val_loss: 0.0748\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0397 - val_loss: 0.0854\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0395 - val_loss: 0.0801\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0390 - val_loss: 0.1133\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0385 - val_loss: 0.0803\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0387 - val_loss: 0.0949\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 53s 254ms/step - loss: 0.1116 - val_loss: 0.1210\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0924 - val_loss: 0.1073\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0911 - val_loss: 0.1088\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0926 - val_loss: 0.1276\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0908 - val_loss: 0.0963\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0923 - val_loss: 0.1422\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0913 - val_loss: 0.0911\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0920 - val_loss: 0.1227\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0922 - val_loss: 0.1072\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0911 - val_loss: 0.1090\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0926 - val_loss: 0.1274\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0908 - val_loss: 0.0964\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0923 - val_loss: 0.1420\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0913 - val_loss: 0.0913\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0920 - val_loss: 0.1225\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0922 - val_loss: 0.1073\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0911 - val_loss: 0.1090\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0926 - val_loss: 0.1275\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0908 - val_loss: 0.0964\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 52s 262ms/step - loss: 0.0923 - val_loss: 0.1423\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0904 - val_loss: 0.0788\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0829 - val_loss: 0.0721\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0781 - val_loss: 0.0956\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0742 - val_loss: 0.0659\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0745 - val_loss: 0.1134\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0721 - val_loss: 0.0621\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0730 - val_loss: 0.0930\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0715 - val_loss: 0.0692\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0714 - val_loss: 0.0785\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0709 - val_loss: 0.0905\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0692 - val_loss: 0.0622\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0700 - val_loss: 0.0815\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0681 - val_loss: 0.0647\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0688 - val_loss: 0.1022\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0692 - val_loss: 0.0643\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0692 - val_loss: 0.0714\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0697 - val_loss: 0.1013\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0686 - val_loss: 0.0675\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0688 - val_loss: 0.0844\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0698 - val_loss: 0.0753\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0691 - val_loss: 0.1079\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0696 - val_loss: 0.0690\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0683 - val_loss: 0.0757\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0680 - val_loss: 0.0835\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0673 - val_loss: 0.0722\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0674 - val_loss: 0.0757\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0651 - val_loss: 0.0708\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0658 - val_loss: 0.1100\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0656 - val_loss: 0.0840\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0653 - val_loss: 0.0848\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 55s 263ms/step - loss: 0.1365 - val_loss: 0.0926\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0851 - val_loss: 0.1210\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0697 - val_loss: 0.0645\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0660 - val_loss: 0.0963\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0621 - val_loss: 0.0578\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0604 - val_loss: 0.0819\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0587 - val_loss: 0.0728\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0585 - val_loss: 0.0679\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0580 - val_loss: 0.0776\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0568 - val_loss: 0.0771\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0578 - val_loss: 0.0928\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0555 - val_loss: 0.0743\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0550 - val_loss: 0.0923\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0546 - val_loss: 0.0898\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0567 - val_loss: 0.0857\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 51s 253ms/step - loss: 0.0589 - val_loss: 0.0656\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0549 - val_loss: 0.0794\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0543 - val_loss: 0.0847\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0524 - val_loss: 0.0704\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0517 - val_loss: 0.0996\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0514 - val_loss: 0.0750\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0504 - val_loss: 0.0988\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0506 - val_loss: 0.0855\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0494 - val_loss: 0.0996\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0491 - val_loss: 0.0919\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0481 - val_loss: 0.0873\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0485 - val_loss: 0.1081\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0478 - val_loss: 0.0958\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0467 - val_loss: 0.1030\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0461 - val_loss: 0.0999\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0459 - val_loss: 0.0919\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0451 - val_loss: 0.0769\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0437 - val_loss: 0.0683\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 51s 254ms/step - loss: 0.0439 - val_loss: 0.1005\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0430 - val_loss: 0.0795\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0426 - val_loss: 0.0949\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0416 - val_loss: 0.1028\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0413 - val_loss: 0.0852\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0410 - val_loss: 0.0736\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0407 - val_loss: 0.0780\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0408 - val_loss: 0.1199\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0397 - val_loss: 0.0770\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0398 - val_loss: 0.0963\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0400 - val_loss: 0.0950\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0410 - val_loss: 0.0791\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 54s 268ms/step - loss: 0.0387 - val_loss: 0.1083\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0393 - val_loss: 0.0809\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0389 - val_loss: 0.1029\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0383 - val_loss: 0.0692\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0385 - val_loss: 0.0916\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 54s 255ms/step - loss: 0.1003 - val_loss: 0.0941\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0841 - val_loss: 0.0942\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0841 - val_loss: 0.0942\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0841 - val_loss: 0.0942\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0841 - val_loss: 0.0942\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 53s 263ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0841 - val_loss: 0.0943\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 51s 258ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0841 - val_loss: 0.0940\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 53s 267ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 53s 267ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 51s 258ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 54s 257ms/step - loss: 0.1090 - val_loss: 0.1140\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0820 - val_loss: 0.0926\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0730 - val_loss: 0.0909\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0674 - val_loss: 0.0911\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0651 - val_loss: 0.0871\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0631 - val_loss: 0.0892\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 53s 264ms/step - loss: 0.0613 - val_loss: 0.0825\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 51s 255ms/step - loss: 0.0602 - val_loss: 0.0836\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0587 - val_loss: 0.0737\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0575 - val_loss: 0.0660\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 53s 265ms/step - loss: 0.0565 - val_loss: 0.0735\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0554 - val_loss: 0.0961\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0550 - val_loss: 0.0903\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0533 - val_loss: 0.0951\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0528 - val_loss: 0.0922\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 51s 256ms/step - loss: 0.0518 - val_loss: 0.0966\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0506 - val_loss: 0.0955\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 53s 267ms/step - loss: 0.0498 - val_loss: 0.0907\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0490 - val_loss: 0.1009\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 51s 258ms/step - loss: 0.0494 - val_loss: 0.0977\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 51s 258ms/step - loss: 0.0513 - val_loss: 0.0811\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 51s 257ms/step - loss: 0.0483 - val_loss: 0.0948\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0474 - val_loss: 0.1009\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0475 - val_loss: 0.0905\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0473 - val_loss: 0.1023\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0456 - val_loss: 0.1216\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0453 - val_loss: 0.1021\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0451 - val_loss: 0.1068\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0440 - val_loss: 0.1132\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0440 - val_loss: 0.0911\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0490 - val_loss: 0.1277\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0441 - val_loss: 0.0999\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0453 - val_loss: 0.0978\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0430 - val_loss: 0.0920\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0557 - val_loss: 0.0898\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 54s 268ms/step - loss: 0.0431 - val_loss: 0.0859\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0419 - val_loss: 0.0959\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0420 - val_loss: 0.0940\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0412 - val_loss: 0.0950\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0409 - val_loss: 0.1327\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0408 - val_loss: 0.0776\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0414 - val_loss: 0.0907\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0409 - val_loss: 0.0770\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0410 - val_loss: 0.0947\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 54s 268ms/step - loss: 0.0402 - val_loss: 0.0911\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0399 - val_loss: 0.1144\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0405 - val_loss: 0.0888\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0401 - val_loss: 0.1332\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0403 - val_loss: 0.1093\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0401 - val_loss: 0.0930\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 54s 258ms/step - loss: 0.1450 - val_loss: 0.1103\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0832 - val_loss: 0.1080\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0835 - val_loss: 0.1090\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 53s 266ms/step - loss: 0.0835 - val_loss: 0.1090\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0834 - val_loss: 0.1065\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 51s 258ms/step - loss: 0.0785 - val_loss: 0.0885\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0725 - val_loss: 0.0945\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0708 - val_loss: 0.0958\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 54s 272ms/step - loss: 0.0699 - val_loss: 0.0989\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0687 - val_loss: 0.0862\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0675 - val_loss: 0.0935\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 53s 267ms/step - loss: 0.0671 - val_loss: 0.0690\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0663 - val_loss: 0.0772\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0655 - val_loss: 0.0799\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0649 - val_loss: 0.0685\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0641 - val_loss: 0.0635\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0636 - val_loss: 0.0841\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0629 - val_loss: 0.0897\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0610 - val_loss: 0.0781\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0605 - val_loss: 0.0923\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 53s 268ms/step - loss: 0.0597 - val_loss: 0.0885\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0595 - val_loss: 0.0972\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 52s 258ms/step - loss: 0.0584 - val_loss: 0.0725\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0596 - val_loss: 0.0711\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0581 - val_loss: 0.0979\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0572 - val_loss: 0.1047\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.0560 - val_loss: 0.0971\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0559 - val_loss: 0.0921\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 54s 269ms/step - loss: 0.0547 - val_loss: 0.0766\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0545 - val_loss: 0.0882\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0541 - val_loss: 0.0846\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0537 - val_loss: 0.0827\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0529 - val_loss: 0.0837\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0522 - val_loss: 0.0875\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0515 - val_loss: 0.0802\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0516 - val_loss: 0.0943\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 54s 268ms/step - loss: 0.0512 - val_loss: 0.0934\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0514 - val_loss: 0.0891\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0502 - val_loss: 0.0924\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 52s 262ms/step - loss: 0.0497 - val_loss: 0.0867\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0489 - val_loss: 0.0859\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0486 - val_loss: 0.0890\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 52s 260ms/step - loss: 0.0482 - val_loss: 0.0864\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 52s 261ms/step - loss: 0.0469 - val_loss: 0.0839\n",
            "Epoch 45/50\n",
            "154/200 [======================>.......] - ETA: 10s - loss: 0.0471"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_picks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "5mcgPRTL0ODn",
        "outputId": "1142b9a2-089a-40a6-9f55-983609b3f1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dad307c9c8f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrect_picks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'correct_picks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "ePLbVBoW0Rjx",
        "outputId": "e646d19c-54b5-4feb-ebb3-fe359e4e60db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2b3b7f6975d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'value' is not defined"
          ]
        }
      ]
    }
  ]
}