{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Based on notes in this notebook, I have change scaling the data by using MinMax Scaler in future notebooks, which improved model significantly."
      ],
      "metadata": {
        "id": "nbEHozQCza-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from itertools import chain\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "df = pd.read_csv(\"drive/MyDrive/Engineer's Project/test.csv\")"
      ],
      "metadata": {
        "id": "IpOuVnds_MVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jLeGHhQlgDTq",
        "outputId": "f3907e1a-6eec-4dc9-8db4-65d134f33998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Opening    High     Low   Closing  Momentum   Range      ohlc\n",
              "0  2.879833  2.8879  2.8701  2.879821  0.000011  0.0178  2.879414\n",
              "1  2.884751  2.8941  2.8741  2.884744  0.000007  0.0200  2.884424\n",
              "2  2.922487  2.9499  2.8787  2.922526 -0.000040  0.0712  2.918403\n",
              "3  2.924881  2.9462  2.9071  2.924866  0.000015  0.0391  2.925762\n",
              "4  2.889930  2.9105  2.8762  2.889919  0.000011  0.0343  2.891637"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0ba116d-68d1-4259-9be9-89c908897016\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Opening</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Closing</th>\n",
              "      <th>Momentum</th>\n",
              "      <th>Range</th>\n",
              "      <th>ohlc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.879833</td>\n",
              "      <td>2.8879</td>\n",
              "      <td>2.8701</td>\n",
              "      <td>2.879821</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.0178</td>\n",
              "      <td>2.879414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.884751</td>\n",
              "      <td>2.8941</td>\n",
              "      <td>2.8741</td>\n",
              "      <td>2.884744</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.0200</td>\n",
              "      <td>2.884424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.922487</td>\n",
              "      <td>2.9499</td>\n",
              "      <td>2.8787</td>\n",
              "      <td>2.922526</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>0.0712</td>\n",
              "      <td>2.918403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.924881</td>\n",
              "      <td>2.9462</td>\n",
              "      <td>2.9071</td>\n",
              "      <td>2.924866</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.0391</td>\n",
              "      <td>2.925762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.889930</td>\n",
              "      <td>2.9105</td>\n",
              "      <td>2.8762</td>\n",
              "      <td>2.889919</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.0343</td>\n",
              "      <td>2.891637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0ba116d-68d1-4259-9be9-89c908897016')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0ba116d-68d1-4259-9be9-89c908897016 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0ba116d-68d1-4259-9be9-89c908897016');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.pop('Momentum')\n",
        "df.pop('Range')\n",
        "df.pop('ohlc')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcZFxeMsgHQn",
        "outputId": "5ffcca90-3084-4142-e24e-4db99415e013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2.879414\n",
              "1       2.884424\n",
              "2       2.918403\n",
              "3       2.925762\n",
              "4       2.891637\n",
              "          ...   \n",
              "3537    4.293774\n",
              "3538    4.273808\n",
              "3539    4.284615\n",
              "3540    4.223919\n",
              "3541    4.172130\n",
              "Name: ohlc, Length: 3542, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.pop('usa_cpi')\n",
        "df.pop('pol_cpi')\n",
        "df.pop('usa_inter')\n",
        "df.pop('pol_inter')"
      ],
      "metadata": {
        "id": "NwfOpPFd_NKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a9102a-d16f-4742-f287-dc89b45ccb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       5.82\n",
              "1       5.82\n",
              "2       5.82\n",
              "3       5.82\n",
              "4       5.82\n",
              "        ... \n",
              "3537    4.83\n",
              "3538    4.83\n",
              "3539    4.83\n",
              "3540    4.83\n",
              "3541    4.83\n",
              "Name: pol_inter, Length: 3542, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.pop('Date')\n",
        "float_data = np.array(df).astype('float32')\n",
        "\n",
        "#X_std = (float_data - float_data.min(axis=0)) / (float_data.max(axis=0) - float_data.min(axis=0))\n",
        "#X_scaled = X_std * (max - min) + min\n",
        "#float_data['Momentum']\n",
        "mean = float_data[:2501].mean(axis=0)\n",
        "std = float_data[:2501].std(axis=0)\n",
        "float_data -= mean\n",
        "float_data /= std"
      ],
      "metadata": {
        "id": "bq9dTqPN_NRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback = 20\n",
        "step = 1\n",
        "delay = 1\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "t8oSnPHI_NWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,shuffle=False, batch_size=128, step=1):\n",
        "  if max_index is None:\n",
        "    max_index = len(data) - delay - 1\n",
        "  i = min_index + lookback\n",
        "  while 1:\n",
        "    if shuffle:\n",
        "      rows = np.random.randint(\n",
        "        min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "      if i + batch_size >= max_index:\n",
        "        i = min_index + lookback\n",
        "      rows = np.arange(i, min(i + batch_size, max_index))\n",
        "      i += len(rows)\n",
        "    samples = np.zeros((len(rows),lookback // step,data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      indices = range(rows[j] - lookback, rows[j], step)\n",
        "      samples[j] = data[indices]\n",
        "      targets[j] = data[rows[j] + delay][3]\n",
        "    yield samples, targets"
      ],
      "metadata": {
        "id": "2uAbpoGW_NZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=0,\n",
        "max_index=2501,\n",
        "shuffle=True,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "WZkoJaMz_NcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=2501,\n",
        "max_index=3001,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "XYGwnK1w_Ne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=3001,\n",
        "max_index=3541,\n",
        "step=step,\n",
        "batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Log_Ntxc_i8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_steps = (3001 - 2501 - lookback)\n",
        "test_steps = (len(float_data) - 3001 - lookback)"
      ],
      "metadata": {
        "id": "zytVaBdM_ldv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3gqKzkI_LCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca65d2b0-8fde-49ec-e894-74aa5caa2251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160\n",
            "600/600 [==============================] - 54s 86ms/step - loss: 0.7773 - val_loss: 0.6155\n",
            "Epoch 2/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.2930 - val_loss: 0.1160\n",
            "Epoch 3/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.1166 - val_loss: 0.1451\n",
            "Epoch 4/160\n",
            "600/600 [==============================] - 51s 85ms/step - loss: 0.0826 - val_loss: 0.1004\n",
            "Epoch 5/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0758 - val_loss: 0.1290\n",
            "Epoch 6/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0725 - val_loss: 0.1171\n",
            "Epoch 7/160\n",
            "600/600 [==============================] - 51s 85ms/step - loss: 0.0712 - val_loss: 0.1186\n",
            "Epoch 8/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0693 - val_loss: 0.1400\n",
            "Epoch 9/160\n",
            "600/600 [==============================] - 51s 85ms/step - loss: 0.0690 - val_loss: 0.1309\n",
            "Epoch 10/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0683 - val_loss: 0.1263\n",
            "Epoch 11/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0677 - val_loss: 0.1289\n",
            "Epoch 12/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0673 - val_loss: 0.1062\n",
            "Epoch 13/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0667 - val_loss: 0.1064\n",
            "Epoch 14/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0669 - val_loss: 0.1126\n",
            "Epoch 15/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0667 - val_loss: 0.1118\n",
            "Epoch 16/160\n",
            "600/600 [==============================] - 51s 86ms/step - loss: 0.0665 - val_loss: 0.1073\n",
            "Epoch 17/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0660 - val_loss: 0.1007\n",
            "Epoch 18/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0662 - val_loss: 0.0993\n",
            "Epoch 19/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0657 - val_loss: 0.0999\n",
            "Epoch 20/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0660 - val_loss: 0.1013\n",
            "Epoch 21/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0658 - val_loss: 0.0741\n",
            "Epoch 22/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0655 - val_loss: 0.0926\n",
            "Epoch 23/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0650 - val_loss: 0.0928\n",
            "Epoch 24/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0655 - val_loss: 0.0923\n",
            "Epoch 25/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0651 - val_loss: 0.0894\n",
            "Epoch 26/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0649 - val_loss: 0.1129\n",
            "Epoch 27/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0651 - val_loss: 0.1025\n",
            "Epoch 28/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0647 - val_loss: 0.1204\n",
            "Epoch 29/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0646 - val_loss: 0.0816\n",
            "Epoch 30/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0642 - val_loss: 0.0792\n",
            "Epoch 31/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0649 - val_loss: 0.0935\n",
            "Epoch 32/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0643 - val_loss: 0.0879\n",
            "Epoch 33/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0645 - val_loss: 0.0873\n",
            "Epoch 34/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0643 - val_loss: 0.0827\n",
            "Epoch 35/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0640 - val_loss: 0.0815\n",
            "Epoch 36/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0640 - val_loss: 0.0759\n",
            "Epoch 37/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0639 - val_loss: 0.0791\n",
            "Epoch 38/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0636 - val_loss: 0.0701\n",
            "Epoch 39/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0637 - val_loss: 0.0702\n",
            "Epoch 40/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0636 - val_loss: 0.0661\n",
            "Epoch 41/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0641 - val_loss: 0.0622\n",
            "Epoch 42/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0639 - val_loss: 0.0674\n",
            "Epoch 43/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0637 - val_loss: 0.0617\n",
            "Epoch 44/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0633 - val_loss: 0.0628\n",
            "Epoch 45/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0634 - val_loss: 0.0632\n",
            "Epoch 46/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0633 - val_loss: 0.0649\n",
            "Epoch 47/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0631 - val_loss: 0.0593\n",
            "Epoch 48/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0630 - val_loss: 0.0575\n",
            "Epoch 49/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0632 - val_loss: 0.0731\n",
            "Epoch 50/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0632 - val_loss: 0.0695\n",
            "Epoch 51/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0629 - val_loss: 0.0673\n",
            "Epoch 52/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0631 - val_loss: 0.0668\n",
            "Epoch 53/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0629 - val_loss: 0.0682\n",
            "Epoch 54/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0626 - val_loss: 0.0650\n",
            "Epoch 55/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0628 - val_loss: 0.0619\n",
            "Epoch 56/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0629 - val_loss: 0.0628\n",
            "Epoch 57/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0627 - val_loss: 0.0673\n",
            "Epoch 58/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0625 - val_loss: 0.0648\n",
            "Epoch 59/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0628 - val_loss: 0.0709\n",
            "Epoch 60/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0623 - val_loss: 0.0697\n",
            "Epoch 61/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0624 - val_loss: 0.0711\n",
            "Epoch 62/160\n",
            "600/600 [==============================] - 69s 115ms/step - loss: 0.0620 - val_loss: 0.0763\n",
            "Epoch 63/160\n",
            "600/600 [==============================] - 55s 92ms/step - loss: 0.0615 - val_loss: 0.0698\n",
            "Epoch 64/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0620 - val_loss: 0.0682\n",
            "Epoch 65/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0619 - val_loss: 0.0696\n",
            "Epoch 66/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0620 - val_loss: 0.0675\n",
            "Epoch 67/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0623 - val_loss: 0.0661\n",
            "Epoch 68/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0618 - val_loss: 0.0661\n",
            "Epoch 69/160\n",
            "600/600 [==============================] - 53s 89ms/step - loss: 0.0623 - val_loss: 0.0693\n",
            "Epoch 70/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0622 - val_loss: 0.0781\n",
            "Epoch 71/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0615 - val_loss: 0.0694\n",
            "Epoch 72/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0619 - val_loss: 0.0710\n",
            "Epoch 73/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0620 - val_loss: 0.0755\n",
            "Epoch 74/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0619 - val_loss: 0.0708\n",
            "Epoch 75/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0616 - val_loss: 0.0736\n",
            "Epoch 76/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0615 - val_loss: 0.0713\n",
            "Epoch 77/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0614 - val_loss: 0.0756\n",
            "Epoch 78/160\n",
            "600/600 [==============================] - 57s 94ms/step - loss: 0.0616 - val_loss: 0.0770\n",
            "Epoch 79/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0614 - val_loss: 0.0752\n",
            "Epoch 80/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0615 - val_loss: 0.0761\n",
            "Epoch 81/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0615 - val_loss: 0.0758\n",
            "Epoch 82/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0615 - val_loss: 0.0732\n",
            "Epoch 83/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0612 - val_loss: 0.0795\n",
            "Epoch 84/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0609 - val_loss: 0.0726\n",
            "Epoch 85/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0612 - val_loss: 0.0686\n",
            "Epoch 86/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0613 - val_loss: 0.0760\n",
            "Epoch 87/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0613 - val_loss: 0.0771\n",
            "Epoch 88/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0611 - val_loss: 0.0675\n",
            "Epoch 89/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0612 - val_loss: 0.0716\n",
            "Epoch 90/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0608 - val_loss: 0.0732\n",
            "Epoch 91/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0610 - val_loss: 0.0730\n",
            "Epoch 92/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0610 - val_loss: 0.0687\n",
            "Epoch 93/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0610 - val_loss: 0.0725\n",
            "Epoch 94/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0608 - val_loss: 0.0813\n",
            "Epoch 95/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0608 - val_loss: 0.0743\n",
            "Epoch 96/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0606 - val_loss: 0.0774\n",
            "Epoch 97/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0605 - val_loss: 0.0901\n",
            "Epoch 98/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0607 - val_loss: 0.0734\n",
            "Epoch 99/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0610 - val_loss: 0.0826\n",
            "Epoch 100/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0607 - val_loss: 0.0726\n",
            "Epoch 101/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0605 - val_loss: 0.0732\n",
            "Epoch 102/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0604 - val_loss: 0.0738\n",
            "Epoch 103/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0604 - val_loss: 0.0847\n",
            "Epoch 104/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0603 - val_loss: 0.0756\n",
            "Epoch 105/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0606 - val_loss: 0.0750\n",
            "Epoch 106/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0603 - val_loss: 0.0785\n",
            "Epoch 107/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0601 - val_loss: 0.0780\n",
            "Epoch 108/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0602 - val_loss: 0.0715\n",
            "Epoch 109/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0606 - val_loss: 0.0752\n",
            "Epoch 110/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0602 - val_loss: 0.0742\n",
            "Epoch 111/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0602 - val_loss: 0.0759\n",
            "Epoch 112/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0602 - val_loss: 0.0787\n",
            "Epoch 113/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0603 - val_loss: 0.0706\n",
            "Epoch 114/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0600 - val_loss: 0.0779\n",
            "Epoch 115/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0599 - val_loss: 0.0800\n",
            "Epoch 116/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0597 - val_loss: 0.0776\n",
            "Epoch 117/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0595 - val_loss: 0.0761\n",
            "Epoch 118/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0601 - val_loss: 0.0752\n",
            "Epoch 119/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0598 - val_loss: 0.0745\n",
            "Epoch 120/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0599 - val_loss: 0.0862\n",
            "Epoch 121/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0597 - val_loss: 0.0871\n",
            "Epoch 122/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0599 - val_loss: 0.0747\n",
            "Epoch 123/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0596 - val_loss: 0.0766\n",
            "Epoch 124/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0595 - val_loss: 0.0778\n",
            "Epoch 125/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0592 - val_loss: 0.0791\n",
            "Epoch 126/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0596 - val_loss: 0.0844\n",
            "Epoch 127/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0592 - val_loss: 0.0845\n",
            "Epoch 128/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0598 - val_loss: 0.0768\n",
            "Epoch 129/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0595 - val_loss: 0.0894\n",
            "Epoch 130/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0593 - val_loss: 0.0800\n",
            "Epoch 131/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0591 - val_loss: 0.0821\n",
            "Epoch 132/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0593 - val_loss: 0.0785\n",
            "Epoch 133/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0594 - val_loss: 0.0859\n",
            "Epoch 134/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0593 - val_loss: 0.0739\n",
            "Epoch 135/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0592 - val_loss: 0.0857\n",
            "Epoch 136/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0592 - val_loss: 0.0828\n",
            "Epoch 137/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0590 - val_loss: 0.0747\n",
            "Epoch 138/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0591 - val_loss: 0.0766\n",
            "Epoch 139/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0589 - val_loss: 0.0869\n",
            "Epoch 140/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0590 - val_loss: 0.0823\n",
            "Epoch 141/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0589 - val_loss: 0.0837\n",
            "Epoch 142/160\n",
            "600/600 [==============================] - 52s 86ms/step - loss: 0.0589 - val_loss: 0.0783\n",
            "Epoch 143/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0590 - val_loss: 0.0877\n",
            "Epoch 144/160\n",
            "600/600 [==============================] - 58s 97ms/step - loss: 0.0585 - val_loss: 0.0761\n",
            "Epoch 145/160\n",
            "600/600 [==============================] - 53s 89ms/step - loss: 0.0590 - val_loss: 0.0794\n",
            "Epoch 146/160\n",
            "600/600 [==============================] - 54s 90ms/step - loss: 0.0586 - val_loss: 0.0806\n",
            "Epoch 147/160\n",
            "600/600 [==============================] - 53s 89ms/step - loss: 0.0588 - val_loss: 0.0869\n",
            "Epoch 148/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0583 - val_loss: 0.0861\n",
            "Epoch 149/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0584 - val_loss: 0.0896\n",
            "Epoch 150/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0584 - val_loss: 0.0895\n",
            "Epoch 151/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0588 - val_loss: 0.0945\n",
            "Epoch 152/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0588 - val_loss: 0.0899\n",
            "Epoch 153/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0583 - val_loss: 0.0927\n",
            "Epoch 154/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0582 - val_loss: 0.0860\n",
            "Epoch 155/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0583 - val_loss: 0.0890\n",
            "Epoch 156/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0583 - val_loss: 0.0933\n",
            "Epoch 157/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0585 - val_loss: 0.0944\n",
            "Epoch 158/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0583 - val_loss: 0.0802\n",
            "Epoch 159/160\n",
            "600/600 [==============================] - 53s 88ms/step - loss: 0.0581 - val_loss: 0.0858\n",
            "Epoch 160/160\n",
            "600/600 [==============================] - 52s 87ms/step - loss: 0.0580 - val_loss: 0.0898\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(layers.LSTM(128,\n",
        "  activation='softmax',\n",
        "  dropout=0.05,\n",
        "  recurrent_dropout=0.05,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(11,\n",
        "  activation='softmax',))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer= tf.keras.optimizers.RMSprop(), loss='mae')\n",
        "history = model.fit(train_gen,\n",
        "  steps_per_epoch=600,\n",
        "  epochs=160,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model.evaluate(test_gen, steps = test_steps)\n",
        "eval *std[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptUM6caHcS8y",
        "outputId": "a1b80bb3-c818-48cb-95ff-d7175786ba37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521/521 [==============================] - 9s 16ms/step - loss: 0.0742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.027716615532226374"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss,softmax, RMSprop,  CLOSING PRICE PREDICTION')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "O7w8VqDccU7O",
        "outputId": "f10f45c2-91ff-4ea2-bb2d-5e6581de9e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model1"
      ],
      "metadata": {
        "id": "laSGxPFJNQRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try MinMax scaler <br>\n",
        "Architecure beneath <br>\n"
      ],
      "metadata": {
        "id": "K-M5MxHwty2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.models.Sequential()\n",
        "model2.add(layers.LSTM(5,\n",
        "  dropout=0.1,\n",
        "  recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model2.add(layers.Dense(1))\n",
        "model2.compile(optimizer= tf.keras.optimizers.RMSprop(), loss='mae')\n",
        "history2 = model2.fit(train_gen,\n",
        "  steps_per_epoch=200,\n",
        "  epochs=500,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPRzQR_SfsUi",
        "outputId": "8c4fd309-8c58-4617-c150-8253f6e553f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "200/200 [==============================] - 9s 31ms/step - loss: 0.3853 - val_loss: 0.0743\n",
            "Epoch 2/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.1289 - val_loss: 0.0700\n",
            "Epoch 3/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.1071 - val_loss: 0.0597\n",
            "Epoch 4/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.1023 - val_loss: 0.0583\n",
            "Epoch 5/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0990 - val_loss: 0.0568\n",
            "Epoch 6/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0961 - val_loss: 0.0562\n",
            "Epoch 7/500\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.0924 - val_loss: 0.0582\n",
            "Epoch 8/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0910 - val_loss: 0.0537\n",
            "Epoch 9/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0894 - val_loss: 0.0626\n",
            "Epoch 10/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0877 - val_loss: 0.0528\n",
            "Epoch 11/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0869 - val_loss: 0.0519\n",
            "Epoch 12/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0868 - val_loss: 0.0546\n",
            "Epoch 13/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0865 - val_loss: 0.0561\n",
            "Epoch 14/500\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.0865 - val_loss: 0.0528\n",
            "Epoch 15/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0856 - val_loss: 0.0543\n",
            "Epoch 16/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0856 - val_loss: 0.0511\n",
            "Epoch 17/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0841 - val_loss: 0.0510\n",
            "Epoch 18/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0840 - val_loss: 0.0513\n",
            "Epoch 19/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0830 - val_loss: 0.0569\n",
            "Epoch 20/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0831 - val_loss: 0.0512\n",
            "Epoch 21/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0832 - val_loss: 0.0514\n",
            "Epoch 22/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0831 - val_loss: 0.0546\n",
            "Epoch 23/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0822 - val_loss: 0.0535\n",
            "Epoch 24/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0821 - val_loss: 0.0511\n",
            "Epoch 25/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0814 - val_loss: 0.0520\n",
            "Epoch 26/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0816 - val_loss: 0.0505\n",
            "Epoch 27/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0816 - val_loss: 0.0555\n",
            "Epoch 28/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0827 - val_loss: 0.0509\n",
            "Epoch 29/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0812 - val_loss: 0.0506\n",
            "Epoch 30/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0809 - val_loss: 0.0530\n",
            "Epoch 31/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0801 - val_loss: 0.0554\n",
            "Epoch 32/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0805 - val_loss: 0.0531\n",
            "Epoch 33/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0796 - val_loss: 0.0507\n",
            "Epoch 34/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0810 - val_loss: 0.0517\n",
            "Epoch 35/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0803 - val_loss: 0.0500\n",
            "Epoch 36/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0801 - val_loss: 0.0530\n",
            "Epoch 37/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0810 - val_loss: 0.0514\n",
            "Epoch 38/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0800 - val_loss: 0.0516\n",
            "Epoch 39/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0801 - val_loss: 0.0513\n",
            "Epoch 40/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0806 - val_loss: 0.0525\n",
            "Epoch 41/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0798 - val_loss: 0.0498\n",
            "Epoch 42/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0801 - val_loss: 0.0513\n",
            "Epoch 43/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0803 - val_loss: 0.0528\n",
            "Epoch 44/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0797 - val_loss: 0.0505\n",
            "Epoch 45/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0802 - val_loss: 0.0504\n",
            "Epoch 46/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0799 - val_loss: 0.0519\n",
            "Epoch 47/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0795 - val_loss: 0.0527\n",
            "Epoch 48/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0789 - val_loss: 0.0500\n",
            "Epoch 49/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0794 - val_loss: 0.0514\n",
            "Epoch 50/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0791 - val_loss: 0.0530\n",
            "Epoch 51/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0795 - val_loss: 0.0503\n",
            "Epoch 52/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0794 - val_loss: 0.0507\n",
            "Epoch 53/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0798 - val_loss: 0.0499\n",
            "Epoch 54/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0802 - val_loss: 0.0504\n",
            "Epoch 55/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0797 - val_loss: 0.0498\n",
            "Epoch 56/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0794 - val_loss: 0.0500\n",
            "Epoch 57/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0789 - val_loss: 0.0503\n",
            "Epoch 58/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0781 - val_loss: 0.0507\n",
            "Epoch 59/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0784 - val_loss: 0.0497\n",
            "Epoch 60/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0793 - val_loss: 0.0503\n",
            "Epoch 61/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0797 - val_loss: 0.0517\n",
            "Epoch 62/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0793 - val_loss: 0.0496\n",
            "Epoch 63/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0790 - val_loss: 0.0527\n",
            "Epoch 64/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0786 - val_loss: 0.0508\n",
            "Epoch 65/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0794 - val_loss: 0.0523\n",
            "Epoch 66/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0793 - val_loss: 0.0510\n",
            "Epoch 67/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0785 - val_loss: 0.0519\n",
            "Epoch 68/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0796 - val_loss: 0.0510\n",
            "Epoch 69/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0787 - val_loss: 0.0509\n",
            "Epoch 70/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0786 - val_loss: 0.0506\n",
            "Epoch 71/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0793 - val_loss: 0.0511\n",
            "Epoch 72/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0782 - val_loss: 0.0541\n",
            "Epoch 73/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0792 - val_loss: 0.0495\n",
            "Epoch 74/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0798 - val_loss: 0.0497\n",
            "Epoch 75/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0787 - val_loss: 0.0496\n",
            "Epoch 76/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0789 - val_loss: 0.0500\n",
            "Epoch 77/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0784 - val_loss: 0.0496\n",
            "Epoch 78/500\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.0777 - val_loss: 0.0520\n",
            "Epoch 79/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0781 - val_loss: 0.0535\n",
            "Epoch 80/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0788 - val_loss: 0.0495\n",
            "Epoch 81/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0791 - val_loss: 0.0503\n",
            "Epoch 82/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0786 - val_loss: 0.0500\n",
            "Epoch 83/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0785 - val_loss: 0.0527\n",
            "Epoch 84/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0784 - val_loss: 0.0532\n",
            "Epoch 85/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0780 - val_loss: 0.0496\n",
            "Epoch 86/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0782 - val_loss: 0.0493\n",
            "Epoch 87/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0783 - val_loss: 0.0493\n",
            "Epoch 88/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0787 - val_loss: 0.0497\n",
            "Epoch 89/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0783 - val_loss: 0.0516\n",
            "Epoch 90/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0785 - val_loss: 0.0527\n",
            "Epoch 91/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0783 - val_loss: 0.0501\n",
            "Epoch 92/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0778 - val_loss: 0.0536\n",
            "Epoch 93/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0492\n",
            "Epoch 94/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0781 - val_loss: 0.0505\n",
            "Epoch 95/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0787 - val_loss: 0.0500\n",
            "Epoch 96/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0781 - val_loss: 0.0542\n",
            "Epoch 97/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0784 - val_loss: 0.0520\n",
            "Epoch 98/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0785 - val_loss: 0.0504\n",
            "Epoch 99/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0776 - val_loss: 0.0534\n",
            "Epoch 100/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0783 - val_loss: 0.0509\n",
            "Epoch 101/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0782 - val_loss: 0.0499\n",
            "Epoch 102/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0775 - val_loss: 0.0524\n",
            "Epoch 103/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0781 - val_loss: 0.0557\n",
            "Epoch 104/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0781 - val_loss: 0.0500\n",
            "Epoch 105/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0786 - val_loss: 0.0515\n",
            "Epoch 106/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0781 - val_loss: 0.0491\n",
            "Epoch 107/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0779 - val_loss: 0.0492\n",
            "Epoch 108/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0785 - val_loss: 0.0543\n",
            "Epoch 109/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0778 - val_loss: 0.0508\n",
            "Epoch 110/500\n",
            "200/200 [==============================] - 10s 51ms/step - loss: 0.0779 - val_loss: 0.0499\n",
            "Epoch 111/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0779 - val_loss: 0.0496\n",
            "Epoch 112/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0782 - val_loss: 0.0498\n",
            "Epoch 113/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0780 - val_loss: 0.0502\n",
            "Epoch 114/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0783 - val_loss: 0.0491\n",
            "Epoch 115/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0785 - val_loss: 0.0493\n",
            "Epoch 116/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0781 - val_loss: 0.0510\n",
            "Epoch 117/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0784 - val_loss: 0.0498\n",
            "Epoch 118/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0489\n",
            "Epoch 119/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0783 - val_loss: 0.0489\n",
            "Epoch 120/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0784 - val_loss: 0.0490\n",
            "Epoch 121/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0781 - val_loss: 0.0509\n",
            "Epoch 122/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0772 - val_loss: 0.0489\n",
            "Epoch 123/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0785 - val_loss: 0.0505\n",
            "Epoch 124/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0777 - val_loss: 0.0511\n",
            "Epoch 125/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0776 - val_loss: 0.0493\n",
            "Epoch 126/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0771 - val_loss: 0.0489\n",
            "Epoch 127/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0782 - val_loss: 0.0494\n",
            "Epoch 128/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0785 - val_loss: 0.0566\n",
            "Epoch 129/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0782 - val_loss: 0.0486\n",
            "Epoch 130/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0774 - val_loss: 0.0518\n",
            "Epoch 131/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0775 - val_loss: 0.0522\n",
            "Epoch 132/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0517\n",
            "Epoch 133/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0783 - val_loss: 0.0517\n",
            "Epoch 134/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0774 - val_loss: 0.0496\n",
            "Epoch 135/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0777 - val_loss: 0.0486\n",
            "Epoch 136/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0775 - val_loss: 0.0540\n",
            "Epoch 137/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0783 - val_loss: 0.0526\n",
            "Epoch 138/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0778 - val_loss: 0.0492\n",
            "Epoch 139/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0767 - val_loss: 0.0489\n",
            "Epoch 140/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0779 - val_loss: 0.0497\n",
            "Epoch 141/500\n",
            "200/200 [==============================] - 10s 50ms/step - loss: 0.0777 - val_loss: 0.0526\n",
            "Epoch 142/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0777 - val_loss: 0.0494\n",
            "Epoch 143/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0779 - val_loss: 0.0523\n",
            "Epoch 144/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0779 - val_loss: 0.0489\n",
            "Epoch 145/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0783 - val_loss: 0.0498\n",
            "Epoch 146/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0781 - val_loss: 0.0496\n",
            "Epoch 147/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0778 - val_loss: 0.0490\n",
            "Epoch 148/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0781 - val_loss: 0.0496\n",
            "Epoch 149/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0504\n",
            "Epoch 150/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0496\n",
            "Epoch 151/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0778 - val_loss: 0.0486\n",
            "Epoch 152/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0775 - val_loss: 0.0536\n",
            "Epoch 153/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0774 - val_loss: 0.0504\n",
            "Epoch 154/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0776 - val_loss: 0.0489\n",
            "Epoch 155/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0781 - val_loss: 0.0504\n",
            "Epoch 156/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0773 - val_loss: 0.0494\n",
            "Epoch 157/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0492\n",
            "Epoch 158/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0776 - val_loss: 0.0492\n",
            "Epoch 159/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0547\n",
            "Epoch 160/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0777 - val_loss: 0.0491\n",
            "Epoch 161/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0497\n",
            "Epoch 162/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0489\n",
            "Epoch 163/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0776 - val_loss: 0.0494\n",
            "Epoch 164/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0516\n",
            "Epoch 165/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0776 - val_loss: 0.0490\n",
            "Epoch 166/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0781 - val_loss: 0.0491\n",
            "Epoch 167/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0789 - val_loss: 0.0497\n",
            "Epoch 168/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0780 - val_loss: 0.0502\n",
            "Epoch 169/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0558\n",
            "Epoch 170/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0779 - val_loss: 0.0497\n",
            "Epoch 171/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0788 - val_loss: 0.0490\n",
            "Epoch 172/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0495\n",
            "Epoch 173/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0770 - val_loss: 0.0498\n",
            "Epoch 174/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0486\n",
            "Epoch 175/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0770 - val_loss: 0.0488\n",
            "Epoch 176/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0529\n",
            "Epoch 177/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0487\n",
            "Epoch 178/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0535\n",
            "Epoch 179/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0777 - val_loss: 0.0517\n",
            "Epoch 180/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0776 - val_loss: 0.0486\n",
            "Epoch 181/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0503\n",
            "Epoch 182/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0763 - val_loss: 0.0486\n",
            "Epoch 183/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0776 - val_loss: 0.0509\n",
            "Epoch 184/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0772 - val_loss: 0.0490\n",
            "Epoch 185/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0777 - val_loss: 0.0545\n",
            "Epoch 186/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0777 - val_loss: 0.0484\n",
            "Epoch 187/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0486\n",
            "Epoch 188/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0484\n",
            "Epoch 189/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0774 - val_loss: 0.0498\n",
            "Epoch 190/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0772 - val_loss: 0.0483\n",
            "Epoch 191/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0493\n",
            "Epoch 192/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0776 - val_loss: 0.0522\n",
            "Epoch 193/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0775 - val_loss: 0.0489\n",
            "Epoch 194/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0772 - val_loss: 0.0497\n",
            "Epoch 195/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0778 - val_loss: 0.0489\n",
            "Epoch 196/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0774 - val_loss: 0.0497\n",
            "Epoch 197/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0779 - val_loss: 0.0488\n",
            "Epoch 198/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0770 - val_loss: 0.0496\n",
            "Epoch 199/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0509\n",
            "Epoch 200/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0541\n",
            "Epoch 201/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0492\n",
            "Epoch 202/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0773 - val_loss: 0.0504\n",
            "Epoch 203/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0507\n",
            "Epoch 204/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0481\n",
            "Epoch 205/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0764 - val_loss: 0.0512\n",
            "Epoch 206/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0776 - val_loss: 0.0573\n",
            "Epoch 207/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0521\n",
            "Epoch 208/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0779 - val_loss: 0.0491\n",
            "Epoch 209/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0487\n",
            "Epoch 210/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0494\n",
            "Epoch 211/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0772 - val_loss: 0.0486\n",
            "Epoch 212/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0778 - val_loss: 0.0488\n",
            "Epoch 213/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0499\n",
            "Epoch 214/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0776 - val_loss: 0.0491\n",
            "Epoch 215/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0499\n",
            "Epoch 216/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0770 - val_loss: 0.0497\n",
            "Epoch 217/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0776 - val_loss: 0.0493\n",
            "Epoch 218/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0768 - val_loss: 0.0498\n",
            "Epoch 219/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0767 - val_loss: 0.0492\n",
            "Epoch 220/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0771 - val_loss: 0.0492\n",
            "Epoch 221/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0769 - val_loss: 0.0501\n",
            "Epoch 222/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0777 - val_loss: 0.0506\n",
            "Epoch 223/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0771 - val_loss: 0.0490\n",
            "Epoch 224/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0500\n",
            "Epoch 225/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0523\n",
            "Epoch 226/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0487\n",
            "Epoch 227/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0762 - val_loss: 0.0488\n",
            "Epoch 228/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0773 - val_loss: 0.0481\n",
            "Epoch 229/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0771 - val_loss: 0.0496\n",
            "Epoch 230/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0488\n",
            "Epoch 231/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0493\n",
            "Epoch 232/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0780 - val_loss: 0.0514\n",
            "Epoch 233/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0781 - val_loss: 0.0514\n",
            "Epoch 234/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0776 - val_loss: 0.0487\n",
            "Epoch 235/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0774 - val_loss: 0.0503\n",
            "Epoch 236/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0777 - val_loss: 0.0565\n",
            "Epoch 237/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0777 - val_loss: 0.0482\n",
            "Epoch 238/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0487\n",
            "Epoch 239/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0778 - val_loss: 0.0514\n",
            "Epoch 240/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0504\n",
            "Epoch 241/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0783 - val_loss: 0.0489\n",
            "Epoch 242/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0780 - val_loss: 0.0496\n",
            "Epoch 243/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0777 - val_loss: 0.0482\n",
            "Epoch 244/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0774 - val_loss: 0.0495\n",
            "Epoch 245/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0772 - val_loss: 0.0490\n",
            "Epoch 246/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0770 - val_loss: 0.0492\n",
            "Epoch 247/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0763 - val_loss: 0.0484\n",
            "Epoch 248/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0771 - val_loss: 0.0497\n",
            "Epoch 249/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0770 - val_loss: 0.0479\n",
            "Epoch 250/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0780 - val_loss: 0.0510\n",
            "Epoch 251/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0781 - val_loss: 0.0493\n",
            "Epoch 252/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0518\n",
            "Epoch 253/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0779 - val_loss: 0.0484\n",
            "Epoch 254/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0771 - val_loss: 0.0483\n",
            "Epoch 255/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0773 - val_loss: 0.0526\n",
            "Epoch 256/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0772 - val_loss: 0.0490\n",
            "Epoch 257/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0775 - val_loss: 0.0499\n",
            "Epoch 258/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0770 - val_loss: 0.0523\n",
            "Epoch 259/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0484\n",
            "Epoch 260/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0779 - val_loss: 0.0506\n",
            "Epoch 261/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0486\n",
            "Epoch 262/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0768 - val_loss: 0.0482\n",
            "Epoch 263/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0770 - val_loss: 0.0509\n",
            "Epoch 264/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0482\n",
            "Epoch 265/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0775 - val_loss: 0.0481\n",
            "Epoch 266/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0763 - val_loss: 0.0483\n",
            "Epoch 267/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0500\n",
            "Epoch 268/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0478\n",
            "Epoch 269/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0768 - val_loss: 0.0490\n",
            "Epoch 270/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0487\n",
            "Epoch 271/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0780 - val_loss: 0.0509\n",
            "Epoch 272/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0528\n",
            "Epoch 273/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0767 - val_loss: 0.0480\n",
            "Epoch 274/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0778 - val_loss: 0.0499\n",
            "Epoch 275/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0480\n",
            "Epoch 276/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0523\n",
            "Epoch 277/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0491\n",
            "Epoch 278/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0514\n",
            "Epoch 279/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0497\n",
            "Epoch 280/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0774 - val_loss: 0.0482\n",
            "Epoch 281/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0764 - val_loss: 0.0480\n",
            "Epoch 282/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0483\n",
            "Epoch 283/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0492\n",
            "Epoch 284/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0780 - val_loss: 0.0528\n",
            "Epoch 285/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0483\n",
            "Epoch 286/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0768 - val_loss: 0.0481\n",
            "Epoch 287/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0775 - val_loss: 0.0480\n",
            "Epoch 288/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0773 - val_loss: 0.0478\n",
            "Epoch 289/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0776 - val_loss: 0.0525\n",
            "Epoch 290/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0515\n",
            "Epoch 291/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0488\n",
            "Epoch 292/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0777 - val_loss: 0.0482\n",
            "Epoch 293/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0498\n",
            "Epoch 294/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0775 - val_loss: 0.0507\n",
            "Epoch 295/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0774 - val_loss: 0.0503\n",
            "Epoch 296/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0766 - val_loss: 0.0502\n",
            "Epoch 297/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0773 - val_loss: 0.0482\n",
            "Epoch 298/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0499\n",
            "Epoch 299/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0766 - val_loss: 0.0507\n",
            "Epoch 300/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0506\n",
            "Epoch 301/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0476\n",
            "Epoch 302/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0777 - val_loss: 0.0478\n",
            "Epoch 303/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0766 - val_loss: 0.0479\n",
            "Epoch 304/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0772 - val_loss: 0.0482\n",
            "Epoch 305/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0770 - val_loss: 0.0480\n",
            "Epoch 306/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0767 - val_loss: 0.0484\n",
            "Epoch 307/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0763 - val_loss: 0.0477\n",
            "Epoch 308/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0767 - val_loss: 0.0478\n",
            "Epoch 309/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0768 - val_loss: 0.0478\n",
            "Epoch 310/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0774 - val_loss: 0.0498\n",
            "Epoch 311/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0769 - val_loss: 0.0489\n",
            "Epoch 312/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0770 - val_loss: 0.0480\n",
            "Epoch 313/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0765 - val_loss: 0.0547\n",
            "Epoch 314/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0774 - val_loss: 0.0489\n",
            "Epoch 315/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0770 - val_loss: 0.0477\n",
            "Epoch 316/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0781 - val_loss: 0.0494\n",
            "Epoch 317/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0770 - val_loss: 0.0503\n",
            "Epoch 318/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0514\n",
            "Epoch 319/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0767 - val_loss: 0.0489\n",
            "Epoch 320/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0769 - val_loss: 0.0509\n",
            "Epoch 321/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0489\n",
            "Epoch 322/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0487\n",
            "Epoch 323/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0767 - val_loss: 0.0478\n",
            "Epoch 324/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0768 - val_loss: 0.0496\n",
            "Epoch 325/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0528\n",
            "Epoch 326/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0513\n",
            "Epoch 327/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0773 - val_loss: 0.0490\n",
            "Epoch 328/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0768 - val_loss: 0.0527\n",
            "Epoch 329/500\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.0771 - val_loss: 0.0478\n",
            "Epoch 330/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0578\n",
            "Epoch 331/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0478\n",
            "Epoch 332/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0776 - val_loss: 0.0486\n",
            "Epoch 333/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0511\n",
            "Epoch 334/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0771 - val_loss: 0.0483\n",
            "Epoch 335/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0504\n",
            "Epoch 336/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0765 - val_loss: 0.0481\n",
            "Epoch 337/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0768 - val_loss: 0.0479\n",
            "Epoch 338/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0775 - val_loss: 0.0487\n",
            "Epoch 339/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0763 - val_loss: 0.0501\n",
            "Epoch 340/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0560\n",
            "Epoch 341/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0502\n",
            "Epoch 342/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0502\n",
            "Epoch 343/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0763 - val_loss: 0.0505\n",
            "Epoch 344/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0767 - val_loss: 0.0476\n",
            "Epoch 345/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0776 - val_loss: 0.0506\n",
            "Epoch 346/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0649\n",
            "Epoch 347/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0520\n",
            "Epoch 348/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0771 - val_loss: 0.0501\n",
            "Epoch 349/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0765 - val_loss: 0.0495\n",
            "Epoch 350/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0777 - val_loss: 0.0514\n",
            "Epoch 351/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0496\n",
            "Epoch 352/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0503\n",
            "Epoch 353/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0765 - val_loss: 0.0487\n",
            "Epoch 354/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0772 - val_loss: 0.0476\n",
            "Epoch 355/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0770 - val_loss: 0.0483\n",
            "Epoch 356/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0485\n",
            "Epoch 357/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0478\n",
            "Epoch 358/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0534\n",
            "Epoch 359/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0768 - val_loss: 0.0507\n",
            "Epoch 360/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0765 - val_loss: 0.0475\n",
            "Epoch 361/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0770 - val_loss: 0.0506\n",
            "Epoch 362/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0759 - val_loss: 0.0508\n",
            "Epoch 363/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0778 - val_loss: 0.0475\n",
            "Epoch 364/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0510\n",
            "Epoch 365/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0757 - val_loss: 0.0529\n",
            "Epoch 366/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0769 - val_loss: 0.0475\n",
            "Epoch 367/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0776 - val_loss: 0.0482\n",
            "Epoch 368/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0765 - val_loss: 0.0476\n",
            "Epoch 369/500\n",
            "200/200 [==============================] - 6s 33ms/step - loss: 0.0765 - val_loss: 0.0494\n",
            "Epoch 370/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0763 - val_loss: 0.0517\n",
            "Epoch 371/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0774 - val_loss: 0.0568\n",
            "Epoch 372/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0495\n",
            "Epoch 373/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0771 - val_loss: 0.0476\n",
            "Epoch 374/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0769 - val_loss: 0.0515\n",
            "Epoch 375/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0768 - val_loss: 0.0478\n",
            "Epoch 376/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0772 - val_loss: 0.0478\n",
            "Epoch 377/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0761 - val_loss: 0.0511\n",
            "Epoch 378/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0497\n",
            "Epoch 379/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0766 - val_loss: 0.0503\n",
            "Epoch 380/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0498\n",
            "Epoch 381/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0509\n",
            "Epoch 382/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0502\n",
            "Epoch 383/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0765 - val_loss: 0.0523\n",
            "Epoch 384/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0762 - val_loss: 0.0517\n",
            "Epoch 385/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0477\n",
            "Epoch 386/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0766 - val_loss: 0.0507\n",
            "Epoch 387/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0518\n",
            "Epoch 388/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0758 - val_loss: 0.0509\n",
            "Epoch 389/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0512\n",
            "Epoch 390/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0537\n",
            "Epoch 391/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0482\n",
            "Epoch 392/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0761 - val_loss: 0.0588\n",
            "Epoch 393/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0773 - val_loss: 0.0528\n",
            "Epoch 394/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0765 - val_loss: 0.0480\n",
            "Epoch 395/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0484\n",
            "Epoch 396/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0545\n",
            "Epoch 397/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0769 - val_loss: 0.0476\n",
            "Epoch 398/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0764 - val_loss: 0.0476\n",
            "Epoch 399/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0555\n",
            "Epoch 400/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0514\n",
            "Epoch 401/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0761 - val_loss: 0.0480\n",
            "Epoch 402/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0767 - val_loss: 0.0480\n",
            "Epoch 403/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0767 - val_loss: 0.0569\n",
            "Epoch 404/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0484\n",
            "Epoch 405/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0772 - val_loss: 0.0512\n",
            "Epoch 406/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0765 - val_loss: 0.0479\n",
            "Epoch 407/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0507\n",
            "Epoch 408/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0766 - val_loss: 0.0480\n",
            "Epoch 409/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0754 - val_loss: 0.0498\n",
            "Epoch 410/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0472\n",
            "Epoch 411/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0762 - val_loss: 0.0512\n",
            "Epoch 412/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0767 - val_loss: 0.0477\n",
            "Epoch 413/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0768 - val_loss: 0.0477\n",
            "Epoch 414/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0766 - val_loss: 0.0485\n",
            "Epoch 415/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0761 - val_loss: 0.0486\n",
            "Epoch 416/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0761 - val_loss: 0.0491\n",
            "Epoch 417/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0770 - val_loss: 0.0510\n",
            "Epoch 418/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0759 - val_loss: 0.0474\n",
            "Epoch 419/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0769 - val_loss: 0.0517\n",
            "Epoch 420/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0769 - val_loss: 0.0510\n",
            "Epoch 421/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0760 - val_loss: 0.0508\n",
            "Epoch 422/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0481\n",
            "Epoch 423/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0772 - val_loss: 0.0474\n",
            "Epoch 424/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0758 - val_loss: 0.0480\n",
            "Epoch 425/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0761 - val_loss: 0.0554\n",
            "Epoch 426/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0758 - val_loss: 0.0510\n",
            "Epoch 427/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0761 - val_loss: 0.0503\n",
            "Epoch 428/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0485\n",
            "Epoch 429/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0476\n",
            "Epoch 430/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0762 - val_loss: 0.0471\n",
            "Epoch 431/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0763 - val_loss: 0.0493\n",
            "Epoch 432/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0760 - val_loss: 0.0488\n",
            "Epoch 433/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0758 - val_loss: 0.0477\n",
            "Epoch 434/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0762 - val_loss: 0.0495\n",
            "Epoch 435/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0766 - val_loss: 0.0488\n",
            "Epoch 436/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0759 - val_loss: 0.0491\n",
            "Epoch 437/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0760 - val_loss: 0.0477\n",
            "Epoch 438/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0769 - val_loss: 0.0512\n",
            "Epoch 439/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0772 - val_loss: 0.0531\n",
            "Epoch 440/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0762 - val_loss: 0.0492\n",
            "Epoch 441/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0762 - val_loss: 0.0502\n",
            "Epoch 442/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0763 - val_loss: 0.0490\n",
            "Epoch 443/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0763 - val_loss: 0.0482\n",
            "Epoch 444/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0756 - val_loss: 0.0488\n",
            "Epoch 445/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0759 - val_loss: 0.0480\n",
            "Epoch 446/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0763 - val_loss: 0.0485\n",
            "Epoch 447/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0760 - val_loss: 0.0501\n",
            "Epoch 448/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0764 - val_loss: 0.0541\n",
            "Epoch 449/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0757 - val_loss: 0.0482\n",
            "Epoch 450/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0752 - val_loss: 0.0531\n",
            "Epoch 451/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0754 - val_loss: 0.0472\n",
            "Epoch 452/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0757 - val_loss: 0.0517\n",
            "Epoch 453/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0750 - val_loss: 0.0497\n",
            "Epoch 454/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0754 - val_loss: 0.0485\n",
            "Epoch 455/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0760 - val_loss: 0.0471\n",
            "Epoch 456/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0750 - val_loss: 0.0521\n",
            "Epoch 457/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0755 - val_loss: 0.0568\n",
            "Epoch 458/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0754 - val_loss: 0.0569\n",
            "Epoch 459/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0760 - val_loss: 0.0485\n",
            "Epoch 460/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0753 - val_loss: 0.0558\n",
            "Epoch 461/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0755 - val_loss: 0.0511\n",
            "Epoch 462/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0754 - val_loss: 0.0499\n",
            "Epoch 463/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0762 - val_loss: 0.0473\n",
            "Epoch 464/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0752 - val_loss: 0.0484\n",
            "Epoch 465/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0754 - val_loss: 0.0495\n",
            "Epoch 466/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0750 - val_loss: 0.0562\n",
            "Epoch 467/500\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.0750 - val_loss: 0.0471\n",
            "Epoch 468/500\n",
            "200/200 [==============================] - 8s 41ms/step - loss: 0.0751 - val_loss: 0.0485\n",
            "Epoch 469/500\n",
            "200/200 [==============================] - 12s 60ms/step - loss: 0.0758 - val_loss: 0.0523\n",
            "Epoch 470/500\n",
            "200/200 [==============================] - 10s 50ms/step - loss: 0.0749 - val_loss: 0.0496\n",
            "Epoch 471/500\n",
            "200/200 [==============================] - 10s 52ms/step - loss: 0.0756 - val_loss: 0.0539\n",
            "Epoch 472/500\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 0.0759 - val_loss: 0.0481\n",
            "Epoch 473/500\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 0.0754 - val_loss: 0.0467\n",
            "Epoch 474/500\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.0748 - val_loss: 0.0497\n",
            "Epoch 475/500\n",
            "200/200 [==============================] - 11s 55ms/step - loss: 0.0754 - val_loss: 0.0555\n",
            "Epoch 476/500\n",
            "200/200 [==============================] - 8s 38ms/step - loss: 0.0754 - val_loss: 0.0518\n",
            "Epoch 477/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0748 - val_loss: 0.0471\n",
            "Epoch 478/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0750 - val_loss: 0.0517\n",
            "Epoch 479/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0749 - val_loss: 0.0543\n",
            "Epoch 480/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0751 - val_loss: 0.0493\n",
            "Epoch 481/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0747 - val_loss: 0.0574\n",
            "Epoch 482/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0747 - val_loss: 0.0555\n",
            "Epoch 483/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0748 - val_loss: 0.0579\n",
            "Epoch 484/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0750 - val_loss: 0.0573\n",
            "Epoch 485/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0747 - val_loss: 0.0478\n",
            "Epoch 486/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0748 - val_loss: 0.0590\n",
            "Epoch 487/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0753 - val_loss: 0.0528\n",
            "Epoch 488/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0749 - val_loss: 0.0473\n",
            "Epoch 489/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0747 - val_loss: 0.0502\n",
            "Epoch 490/500\n",
            "200/200 [==============================] - 9s 45ms/step - loss: 0.0735 - val_loss: 0.0481\n",
            "Epoch 491/500\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0752 - val_loss: 0.0484\n",
            "Epoch 492/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0746 - val_loss: 0.0568\n",
            "Epoch 493/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0748 - val_loss: 0.0527\n",
            "Epoch 494/500\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.0745 - val_loss: 0.0485\n",
            "Epoch 495/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0735 - val_loss: 0.0531\n",
            "Epoch 496/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0739 - val_loss: 0.0559\n",
            "Epoch 497/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0750 - val_loss: 0.0571\n",
            "Epoch 498/500\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.0747 - val_loss: 0.0604\n",
            "Epoch 499/500\n",
            "200/200 [==============================] - 7s 33ms/step - loss: 0.0743 - val_loss: 0.0566\n",
            "Epoch 500/500\n",
            "200/200 [==============================] - 6s 32ms/step - loss: 0.0733 - val_loss: 0.0514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval2 = model2.evaluate(test_gen, steps = test_steps)\n",
        "eval2 *std[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYfT3u38rObM",
        "outputId": "7a0e5446-9654-4052-d87a-f46bfe61e460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521/521 [==============================] - 3s 6ms/step - loss: 0.0734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02740415069869906"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss2 = history2.history['loss']\n",
        "val_loss2 = history2.history['val_loss']\n",
        "epochs2 = range(1, len(loss2) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs2, loss2, 'r', label='Training loss')\n",
        "plt.plot(epochs2, val_loss2, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss,softmax, RMSprop,  CLOSING PRICE PREDICTION')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "iGOyUXfDrQ_I",
        "outputId": "23e68634-ed9b-4e1f-f456-0c2dd16e8ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now previous, but with other features (momentum, cpi..) with lower number of epochs, as model does not improve"
      ],
      "metadata": {
        "id": "Yqs8JYAlrYuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model with other loss functions"
      ],
      "metadata": {
        "id": "QBC-qX1eIJw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(layers.LSTM(128,\n",
        "  activation='softmax',\n",
        "  dropout=0.1,\n",
        "  recurrent_dropout=0.1,\n",
        "  input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(64,\n",
        "  activation='softmax',))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer= tf.keras.optimizers.RMSprop(learning_rate=0.01), loss='huber', metrics=['mae'])\n",
        "history = model.fit(train_gen,\n",
        "  steps_per_epoch=400,\n",
        "  epochs=200,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "G0GLwRDAKQrr",
        "outputId": "0e15b9f3-31e9-4642-f526-335729d25bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "400/400 [==============================] - 46s 108ms/step - loss: 0.0703 - mae: 0.2202 - val_loss: 0.0036 - val_mae: 0.0660\n",
            "Epoch 2/200\n",
            "400/400 [==============================] - 43s 108ms/step - loss: 0.0059 - mae: 0.0842 - val_loss: 0.0029 - val_mae: 0.0605\n",
            "Epoch 3/200\n",
            "400/400 [==============================] - 44s 110ms/step - loss: 0.0056 - mae: 0.0818 - val_loss: 0.0055 - val_mae: 0.0860\n",
            "Epoch 4/200\n",
            "400/400 [==============================] - 43s 109ms/step - loss: 0.0053 - mae: 0.0799 - val_loss: 0.0056 - val_mae: 0.0881\n",
            "Epoch 5/200\n",
            "400/400 [==============================] - 46s 116ms/step - loss: 0.0052 - mae: 0.0788 - val_loss: 0.0069 - val_mae: 0.0977\n",
            "Epoch 6/200\n",
            "135/400 [=========>....................] - ETA: 23s - loss: 0.0052 - mae: 0.0791"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-625d9db2b67d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   validation_steps=val_steps)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "5XyVMqKsLe_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}